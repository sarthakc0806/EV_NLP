{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'transformer-EV-topic-classification'...\n",
      "remote: Enumerating objects: 121, done.\u001b[K\n",
      "remote: Counting objects: 100% (121/121), done.\u001b[K\n",
      "remote: Compressing objects: 100% (109/109), done.\u001b[K\n",
      "remote: Total 121 (delta 39), reused 27 (delta 4), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (121/121), 1.60 MiB | 7.71 MiB/s, done.\n",
      "Resolving deltas: 100% (39/39), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/asensio-lab/transformer-EV-topic-classification.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "transformer-EV-topic-classification/transformers/code/train_bert.py:30: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n",
      "01/26/2022 19:57:53 - INFO - root -   {'run_text': 'multilabel topic classification with freezable layers', 'train_size': -1, 'val_size': -1, 'log_path': PosixPath('transformer-EV-topic-classification/transformers/logs'), 'full_data_dir': PosixPath('transformer-EV-topic-classification/anonymized_data'), 'data_dir': PosixPath('transformer-EV-topic-classification/anonymized_data'), 'task_name': 'topics', 'no_cuda': False, 'bert_model': None, 'output_dir': PosixPath('transformer-EV-topic-classification/transformers/models/output'), 'max_seq_length': 512, 'do_train': True, 'do_eval': True, 'do_lower_case': True, 'train_batch_size': 8, 'eval_batch_size': 8, 'learning_rate': 0.0001, 'num_train_epochs': 20, 'warmup_proportion': 0.0, 'local_rank': -1, 'seed': 42, 'gradient_accumulation_steps': 1, 'optimize_on_cpu': False, 'fp16': False, 'fp16_opt_level': 'O1', 'weight_decay': 0.01, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'max_steps': -1, 'warmup_steps': 500, 'logging_steps': 50, 'eval_all_checkpoints': True, 'overwrite_output_dir': True, 'overwrite_cache': False, 'loss_scale': 128, 'model_name': 'bert-base-cased', 'model_type': 'bert'}\n",
      "01/26/2022 19:57:53 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/azure12ev22/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
      "01/26/2022 19:57:53 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "01/26/2022 19:57:54 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/azure12ev22/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "01/26/2022 19:57:58 - INFO - root -   Writing example 0 of 8521\n",
      "01/26/2022 19:57:59 - INFO - root -   Saving features into cached file transformer-EV-topic-classification/anonymized_data/cache/cached_bert_train_multi_label_512_train_final.csv\n",
      "01/26/2022 19:58:04 - INFO - root -   Writing example 0 of 1065\n",
      "01/26/2022 19:58:04 - INFO - root -   Saving features into cached file transformer-EV-topic-classification/anonymized_data/cache/cached_bert_dev_multi_label_512_valid_final.csv\n",
      "01/26/2022 19:58:04 - INFO - root -   Writing example 0 of 14\n",
      "01/26/2022 19:58:04 - INFO - root -   Saving features into cached file transformer-EV-topic-classification/anonymized_data/cache/cached_bert_test_multi_label_512_test\n",
      "8\n",
      "cuda:0\n",
      "01/26/2022 19:58:04 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/azure12ev22/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
      "01/26/2022 19:58:04 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "01/26/2022 19:58:05 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/bert-base-cased-pytorch_model.bin from cache at /home/azure12ev22/.cache/torch/transformers/d8f11f061e407be64c4d5d7867ee61d1465263e24085cfa26abf183fdc830569.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
      "01/26/2022 19:58:11 - WARNING - transformers.modeling_utils -   Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMultiLabelSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "01/26/2022 19:58:11 - WARNING - transformers.modeling_utils -   Some weights of BertForMultiLabelSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "01/26/2022 19:58:27 - INFO - root -   ***** Running training *****\n",
      "01/26/2022 19:58:27 - INFO - root -     Num examples = 8521\n",
      "01/26/2022 19:58:27 - INFO - root -     Num Epochs = 20\n",
      "01/26/2022 19:58:27 - INFO - root -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "01/26/2022 19:58:27 - INFO - root -     Gradient Accumulation steps = 1\n",
      "01/26/2022 19:58:27 - INFO - root -     Total optimization steps = 10660\n",
      "/home/azure12ev22/anaconda3/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/azure12ev22/anaconda3/lib/python3.8/site-packages/pytorch_lamb/lamb.py:96: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "01/26/2022 20:02:13 - INFO - root -   Running evaluation         \n",
      "01/26/2022 20:02:13 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 20:02:13 - INFO - root -     Batch size = 32\n",
      "01/26/2022 20:02:20 - INFO - root -   eval_loss after epoch 1: 0.4205902735976612: \n",
      "01/26/2022 20:02:20 - INFO - root -   eval_accuracy after epoch 1: 0.0: \n",
      "01/26/2022 20:02:20 - INFO - root -   eval_accuracy_thresh after epoch 1: 0.8386150598526001: \n",
      "01/26/2022 20:02:20 - INFO - root -   eval_roc_auc after epoch 1: 0.7782355775984093: \n",
      "01/26/2022 20:02:20 - INFO - root -   eval_fbeta after epoch 1: 0.5746349096298218: \n",
      "/home/azure12ev22/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "01/26/2022 20:02:20 - INFO - root -   lr after epoch 1: 9.999739698632535e-05\n",
      "01/26/2022 20:02:20 - INFO - root -   train_loss after epoch 1: 0.630753634962311\n",
      "01/26/2022 20:02:20 - INFO - root -   \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/26/2022 20:06:02 - INFO - root -   Running evaluation         \n",
      "01/26/2022 20:06:02 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 20:06:02 - INFO - root -     Batch size = 32\n",
      "01/26/2022 20:06:09 - INFO - root -   eval_loss after epoch 2: 0.3186812979333541: \n",
      "01/26/2022 20:06:09 - INFO - root -   eval_accuracy after epoch 2: 0.0: \n",
      "01/26/2022 20:06:09 - INFO - root -   eval_accuracy_thresh after epoch 2: 0.8803991079330444: \n",
      "01/26/2022 20:06:09 - INFO - root -   eval_roc_auc after epoch 2: 0.89341873858688: \n",
      "01/26/2022 20:06:09 - INFO - root -   eval_fbeta after epoch 2: 0.7273890972137451: \n",
      "01/26/2022 20:06:09 - INFO - root -   lr after epoch 2: 9.923620574858906e-05\n",
      "01/26/2022 20:06:09 - INFO - root -   train_loss after epoch 2: 0.378680688578908\n",
      "01/26/2022 20:06:09 - INFO - root -   \n",
      "\n",
      "01/26/2022 20:09:51 - INFO - root -   Running evaluation         \n",
      "01/26/2022 20:09:51 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 20:09:51 - INFO - root -     Batch size = 32\n",
      "01/26/2022 20:09:58 - INFO - root -   eval_loss after epoch 3: 0.2600090595729211: \n",
      "01/26/2022 20:09:58 - INFO - root -   eval_accuracy after epoch 3: 0.0: \n",
      "01/26/2022 20:09:58 - INFO - root -   eval_accuracy_thresh after epoch 3: 0.9016432166099548: \n",
      "01/26/2022 20:09:58 - INFO - root -   eval_roc_auc after epoch 3: 0.9320804348060986: \n",
      "01/26/2022 20:09:58 - INFO - root -   eval_fbeta after epoch 3: 0.800105631351471: \n",
      "01/26/2022 20:09:58 - INFO - root -   lr after epoch 3: 9.714066971576419e-05\n",
      "01/26/2022 20:09:58 - INFO - root -   train_loss after epoch 3: 0.3006404820850598\n",
      "01/26/2022 20:09:58 - INFO - root -   \n",
      "\n",
      "01/26/2022 20:13:39 - INFO - root -   Running evaluation         \n",
      "01/26/2022 20:13:39 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 20:13:39 - INFO - root -     Batch size = 32\n",
      "01/26/2022 20:13:46 - INFO - root -   eval_loss after epoch 4: 0.23685586759272745: \n",
      "01/26/2022 20:13:46 - INFO - root -   eval_accuracy after epoch 4: 0.0: \n",
      "01/26/2022 20:13:46 - INFO - root -   eval_accuracy_thresh after epoch 4: 0.906455397605896: \n",
      "01/26/2022 20:13:46 - INFO - root -   eval_roc_auc after epoch 4: 0.9413056717127514: \n",
      "01/26/2022 20:13:46 - INFO - root -   eval_fbeta after epoch 4: 0.8021053671836853: \n",
      "01/26/2022 20:13:46 - INFO - root -   lr after epoch 4: 9.376757977081594e-05\n",
      "01/26/2022 20:13:46 - INFO - root -   train_loss after epoch 4: 0.25552090004394024\n",
      "01/26/2022 20:13:46 - INFO - root -   \n",
      "\n",
      "01/26/2022 20:17:27 - INFO - root -   Running evaluation         \n",
      "01/26/2022 20:17:27 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 20:17:27 - INFO - root -     Batch size = 32\n",
      "01/26/2022 20:17:34 - INFO - root -   eval_loss after epoch 5: 0.22921346916871913: \n",
      "01/26/2022 20:17:34 - INFO - root -   eval_accuracy after epoch 5: 0.0: \n",
      "01/26/2022 20:17:34 - INFO - root -   eval_accuracy_thresh after epoch 5: 0.9079812169075012: \n",
      "01/26/2022 20:17:34 - INFO - root -   eval_roc_auc after epoch 5: 0.9436258765816289: \n",
      "01/26/2022 20:17:34 - INFO - root -   eval_fbeta after epoch 5: 0.800257682800293: \n",
      "01/26/2022 20:17:34 - INFO - root -   lr after epoch 5: 8.920834963953769e-05\n",
      "01/26/2022 20:17:34 - INFO - root -   train_loss after epoch 5: 0.22665391834435275\n",
      "01/26/2022 20:17:34 - INFO - root -   \n",
      "\n",
      "01/26/2022 20:21:16 - INFO - root -   Running evaluation         \n",
      "01/26/2022 20:21:16 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 20:21:16 - INFO - root -     Batch size = 32\n",
      "01/26/2022 20:21:23 - INFO - root -   eval_loss after epoch 6: 0.22043892741203308: \n",
      "01/26/2022 20:21:23 - INFO - root -   eval_accuracy after epoch 6: 0.0: \n",
      "01/26/2022 20:21:23 - INFO - root -   eval_accuracy_thresh after epoch 6: 0.9126760959625244: \n",
      "01/26/2022 20:21:23 - INFO - root -   eval_roc_auc after epoch 6: 0.9477368256129318: \n",
      "01/26/2022 20:21:23 - INFO - root -   eval_fbeta after epoch 6: 0.8156130909919739: \n",
      "01/26/2022 20:21:23 - INFO - root -   lr after epoch 6: 8.358653849759678e-05\n",
      "01/26/2022 20:21:23 - INFO - root -   train_loss after epoch 6: 0.207770955607323\n",
      "01/26/2022 20:21:23 - INFO - root -   \n",
      "\n",
      "01/26/2022 20:25:04 - INFO - root -   Running evaluation         \n",
      "01/26/2022 20:25:04 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 20:25:04 - INFO - root -     Batch size = 32\n",
      "01/26/2022 20:25:11 - INFO - root -   eval_loss after epoch 7: 0.21834740743917577: \n",
      "01/26/2022 20:25:11 - INFO - root -   eval_accuracy after epoch 7: 0.0: \n",
      "01/26/2022 20:25:11 - INFO - root -   eval_accuracy_thresh after epoch 7: 0.9117370843887329: \n",
      "01/26/2022 20:25:11 - INFO - root -   eval_roc_auc after epoch 7: 0.9489808205560418: \n",
      "01/26/2022 20:25:11 - INFO - root -   eval_fbeta after epoch 7: 0.8116808533668518: \n",
      "01/26/2022 20:25:11 - INFO - root -   lr after epoch 7: 7.705450240741043e-05\n",
      "01/26/2022 20:25:11 - INFO - root -   train_loss after epoch 7: 0.1902916871612336\n",
      "01/26/2022 20:25:11 - INFO - root -   \n",
      "\n",
      "01/26/2022 20:28:51 - INFO - root -   Running evaluation         \n",
      "01/26/2022 20:28:51 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 20:28:51 - INFO - root -     Batch size = 32\n",
      "01/26/2022 20:28:59 - INFO - root -   eval_loss after epoch 8: 0.215181027265156: \n",
      "01/26/2022 20:28:59 - INFO - root -   eval_accuracy after epoch 8: 0.0: \n",
      "01/26/2022 20:28:59 - INFO - root -   eval_accuracy_thresh after epoch 8: 0.9154930114746094: \n",
      "01/26/2022 20:28:59 - INFO - root -   eval_roc_auc after epoch 8: 0.9503363264071227: \n",
      "01/26/2022 20:28:59 - INFO - root -   eval_fbeta after epoch 8: 0.819844126701355: \n",
      "01/26/2022 20:28:59 - INFO - root -   lr after epoch 8: 6.978926533387911e-05\n",
      "01/26/2022 20:28:59 - INFO - root -   train_loss after epoch 8: 0.17634454533094313\n",
      "01/26/2022 20:28:59 - INFO - root -   \n",
      "\n",
      "01/26/2022 20:32:40 - INFO - root -   Running evaluation         \n",
      "01/26/2022 20:32:40 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 20:32:40 - INFO - root -     Batch size = 32\n",
      "01/26/2022 20:32:47 - INFO - root -   eval_loss after epoch 9: 0.21469683769871206: \n",
      "01/26/2022 20:32:47 - INFO - root -   eval_accuracy after epoch 9: 0.0: \n",
      "01/26/2022 20:32:47 - INFO - root -   eval_accuracy_thresh after epoch 9: 0.9159624576568604: \n",
      "01/26/2022 20:32:47 - INFO - root -   eval_roc_auc after epoch 9: 0.9510503419883951: \n",
      "01/26/2022 20:32:47 - INFO - root -   eval_fbeta after epoch 9: 0.818178653717041: \n",
      "01/26/2022 20:32:47 - INFO - root -   lr after epoch 9: 6.198772163810884e-05\n",
      "01/26/2022 20:32:47 - INFO - root -   train_loss after epoch 9: 0.16395502411681762\n",
      "01/26/2022 20:32:47 - INFO - root -   \n",
      "\n",
      "01/26/2022 20:36:28 - INFO - root -   Running evaluation          \n",
      "01/26/2022 20:36:28 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 20:36:28 - INFO - root -     Batch size = 32\n",
      "01/26/2022 20:36:35 - INFO - root -   eval_loss after epoch 10: 0.21418212880106532: \n",
      "01/26/2022 20:36:35 - INFO - root -   eval_accuracy after epoch 10: 0.0: \n",
      "01/26/2022 20:36:35 - INFO - root -   eval_accuracy_thresh after epoch 10: 0.9158450961112976: \n",
      "01/26/2022 20:36:35 - INFO - root -   eval_roc_auc after epoch 10: 0.9522472257339514: \n",
      "01/26/2022 20:36:35 - INFO - root -   eval_fbeta after epoch 10: 0.825068473815918: \n",
      "01/26/2022 20:36:35 - INFO - root -   lr after epoch 10: 5.38613000657926e-05\n",
      "01/26/2022 20:36:35 - INFO - root -   train_loss after epoch 10: 0.15315344339799256\n",
      "01/26/2022 20:36:35 - INFO - root -   \n",
      "\n",
      "01/26/2022 20:40:16 - INFO - root -   Running evaluation          \n",
      "01/26/2022 20:40:16 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 20:40:16 - INFO - root -     Batch size = 32\n",
      "01/26/2022 20:40:23 - INFO - root -   eval_loss after epoch 11: 0.21650031296645894: \n",
      "01/26/2022 20:40:23 - INFO - root -   eval_accuracy after epoch 11: 0.0: \n",
      "01/26/2022 20:40:23 - INFO - root -   eval_accuracy_thresh after epoch 11: 0.9160798192024231: \n",
      "01/26/2022 20:40:23 - INFO - root -   eval_roc_auc after epoch 11: 0.9516716911407179: \n",
      "01/26/2022 20:40:23 - INFO - root -   eval_fbeta after epoch 11: 0.8168681859970093: \n",
      "01/26/2022 20:40:23 - INFO - root -   lr after epoch 11: 4.5630233840893374e-05\n",
      "01/26/2022 20:40:23 - INFO - root -   train_loss after epoch 11: 0.14331871665329243\n",
      "01/26/2022 20:40:23 - INFO - root -   \n",
      "\n",
      "01/26/2022 20:44:07 - INFO - root -   Running evaluation          \n",
      "01/26/2022 20:44:07 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 20:44:07 - INFO - root -     Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/26/2022 20:44:14 - INFO - root -   eval_loss after epoch 12: 0.21688215303070404: \n",
      "01/26/2022 20:44:14 - INFO - root -   eval_accuracy after epoch 12: 0.0: \n",
      "01/26/2022 20:44:14 - INFO - root -   eval_accuracy_thresh after epoch 12: 0.9154930114746094: \n",
      "01/26/2022 20:44:14 - INFO - root -   eval_roc_auc after epoch 12: 0.9527315202005466: \n",
      "01/26/2022 20:44:14 - INFO - root -   eval_fbeta after epoch 12: 0.8237324357032776: \n",
      "01/26/2022 20:44:14 - INFO - root -   lr after epoch 12: 3.751759215016583e-05\n",
      "01/26/2022 20:44:14 - INFO - root -   train_loss after epoch 12: 0.13559268695244647\n",
      "01/26/2022 20:44:14 - INFO - root -   \n",
      "\n",
      "01/26/2022 20:47:55 - INFO - root -   Running evaluation          \n",
      "01/26/2022 20:47:55 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 20:47:55 - INFO - root -     Batch size = 32\n",
      "01/26/2022 20:48:02 - INFO - root -   eval_loss after epoch 13: 0.22192971233059378: \n",
      "01/26/2022 20:48:02 - INFO - root -   eval_accuracy after epoch 13: 0.0: \n",
      "01/26/2022 20:48:02 - INFO - root -   eval_accuracy_thresh after epoch 13: 0.9153755903244019: \n",
      "01/26/2022 20:48:02 - INFO - root -   eval_roc_auc after epoch 13: 0.9509084897403481: \n",
      "01/26/2022 20:48:02 - INFO - root -   eval_fbeta after epoch 13: 0.8151391744613647: \n",
      "01/26/2022 20:48:02 - INFO - root -   lr after epoch 13: 2.9743234770574034e-05\n",
      "01/26/2022 20:48:02 - INFO - root -   train_loss after epoch 13: 0.12946395317396572\n",
      "01/26/2022 20:48:02 - INFO - root -   \n",
      "\n",
      "01/26/2022 20:51:43 - INFO - root -   Running evaluation          \n",
      "01/26/2022 20:51:43 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 20:51:43 - INFO - root -     Batch size = 32\n",
      "01/26/2022 20:51:50 - INFO - root -   eval_loss after epoch 14: 0.2204482086441096: \n",
      "01/26/2022 20:51:50 - INFO - root -   eval_accuracy after epoch 14: 0.0: \n",
      "01/26/2022 20:51:50 - INFO - root -   eval_accuracy_thresh after epoch 14: 0.9154930114746094: \n",
      "01/26/2022 20:51:50 - INFO - root -   eval_roc_auc after epoch 14: 0.9523107610187256: \n",
      "01/26/2022 20:51:50 - INFO - root -   eval_fbeta after epoch 14: 0.8213633298873901: \n",
      "01/26/2022 20:51:50 - INFO - root -   lr after epoch 14: 2.2517853674557698e-05\n",
      "01/26/2022 20:51:50 - INFO - root -   train_loss after epoch 14: 0.12434946656870126\n",
      "01/26/2022 20:51:50 - INFO - root -   \n",
      "\n",
      "01/26/2022 20:55:31 - INFO - root -   Running evaluation          \n",
      "01/26/2022 20:55:31 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 20:55:31 - INFO - root -     Batch size = 32\n",
      "01/26/2022 20:55:38 - INFO - root -   eval_loss after epoch 15: 0.22195269079769359: \n",
      "01/26/2022 20:55:38 - INFO - root -   eval_accuracy after epoch 15: 0.0: \n",
      "01/26/2022 20:55:38 - INFO - root -   eval_accuracy_thresh after epoch 15: 0.9154930114746094: \n",
      "01/26/2022 20:55:38 - INFO - root -   eval_roc_auc after epoch 15: 0.9520592995991227: \n",
      "01/26/2022 20:55:38 - INFO - root -   eval_fbeta after epoch 15: 0.8216251730918884: \n",
      "01/26/2022 20:55:38 - INFO - root -   lr after epoch 15: 1.6037263090922948e-05\n",
      "01/26/2022 20:55:38 - INFO - root -   train_loss after epoch 15: 0.12023637825396002\n",
      "01/26/2022 20:55:38 - INFO - root -   \n",
      "\n",
      "01/26/2022 20:59:19 - INFO - root -   Running evaluation          \n",
      "01/26/2022 20:59:19 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 20:59:19 - INFO - root -     Batch size = 32\n",
      "01/26/2022 20:59:26 - INFO - root -   eval_loss after epoch 16: 0.22303191923043308: \n",
      "01/26/2022 20:59:26 - INFO - root -   eval_accuracy after epoch 16: 0.0: \n",
      "01/26/2022 20:59:26 - INFO - root -   eval_accuracy_thresh after epoch 16: 0.9164319634437561: \n",
      "01/26/2022 20:59:26 - INFO - root -   eval_roc_auc after epoch 16: 0.951624579943164: \n",
      "01/26/2022 20:59:26 - INFO - root -   eval_fbeta after epoch 16: 0.8213582038879395: \n",
      "01/26/2022 20:59:26 - INFO - root -   lr after epoch 16: 1.0477092765766162e-05\n",
      "01/26/2022 20:59:26 - INFO - root -   train_loss after epoch 16: 0.11741244017826236\n",
      "01/26/2022 20:59:26 - INFO - root -   \n",
      "\n",
      "01/26/2022 21:03:07 - INFO - root -   Running evaluation          \n",
      "01/26/2022 21:03:07 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 21:03:07 - INFO - root -     Batch size = 32\n",
      "01/26/2022 21:03:14 - INFO - root -   eval_loss after epoch 17: 0.22266452978639042: \n",
      "01/26/2022 21:03:14 - INFO - root -   eval_accuracy after epoch 17: 0.0: \n",
      "01/26/2022 21:03:14 - INFO - root -   eval_accuracy_thresh after epoch 17: 0.9158450961112976: \n",
      "01/26/2022 21:03:14 - INFO - root -   eval_roc_auc after epoch 17: 0.9518132840610716: \n",
      "01/26/2022 21:03:14 - INFO - root -   eval_fbeta after epoch 17: 0.8205909132957458: \n",
      "01/26/2022 21:03:14 - INFO - root -   lr after epoch 17: 5.988028240760935e-06\n",
      "01/26/2022 21:03:14 - INFO - root -   train_loss after epoch 17: 0.1150010322722105\n",
      "01/26/2022 21:03:14 - INFO - root -   \n",
      "\n",
      "01/26/2022 21:07:00 - INFO - root -   Running evaluation          \n",
      "01/26/2022 21:07:00 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 21:07:00 - INFO - root -     Batch size = 32\n",
      "01/26/2022 21:07:07 - INFO - root -   eval_loss after epoch 18: 0.22263660851646871: \n",
      "01/26/2022 21:07:07 - INFO - root -   eval_accuracy after epoch 18: 0.0: \n",
      "01/26/2022 21:07:07 - INFO - root -   eval_accuracy_thresh after epoch 18: 0.9156103730201721: \n",
      "01/26/2022 21:07:07 - INFO - root -   eval_roc_auc after epoch 18: 0.9519483073465373: \n",
      "01/26/2022 21:07:07 - INFO - root -   eval_fbeta after epoch 18: 0.821902871131897: \n",
      "01/26/2022 21:07:07 - INFO - root -   lr after epoch 18: 2.6917271414835297e-06\n",
      "01/26/2022 21:07:07 - INFO - root -   train_loss after epoch 18: 0.11363822124604883\n",
      "01/26/2022 21:07:07 - INFO - root -   \n",
      "\n",
      "01/26/2022 21:10:48 - INFO - root -   Running evaluation          \n",
      "01/26/2022 21:10:48 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 21:10:48 - INFO - root -     Batch size = 32\n",
      "01/26/2022 21:10:55 - INFO - root -   eval_loss after epoch 19: 0.22271929199204726: \n",
      "01/26/2022 21:10:55 - INFO - root -   eval_accuracy after epoch 19: 0.0: \n",
      "01/26/2022 21:10:55 - INFO - root -   eval_accuracy_thresh after epoch 19: 0.9152582287788391: \n",
      "01/26/2022 21:10:55 - INFO - root -   eval_roc_auc after epoch 19: 0.9519905777604895: \n",
      "01/26/2022 21:10:55 - INFO - root -   eval_fbeta after epoch 19: 0.8216553330421448: \n",
      "01/26/2022 21:10:55 - INFO - root -   lr after epoch 19: 6.775221479809302e-07\n",
      "01/26/2022 21:10:55 - INFO - root -   train_loss after epoch 19: 0.11321160795541016\n",
      "01/26/2022 21:10:55 - INFO - root -   \n",
      "\n",
      "01/26/2022 21:14:35 - INFO - root -   Running evaluation          \n",
      "01/26/2022 21:14:35 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 21:14:35 - INFO - root -     Batch size = 32\n",
      "01/26/2022 21:14:43 - INFO - root -   eval_loss after epoch 20: 0.22271652475875967: \n",
      "01/26/2022 21:14:43 - INFO - root -   eval_accuracy after epoch 20: 0.0: \n",
      "01/26/2022 21:14:43 - INFO - root -   eval_accuracy_thresh after epoch 20: 0.9152582287788391: \n",
      "01/26/2022 21:14:43 - INFO - root -   eval_roc_auc after epoch 20: 0.9520022475066723: \n",
      "01/26/2022 21:14:43 - INFO - root -   eval_fbeta after epoch 20: 0.8211844563484192: \n",
      "01/26/2022 21:14:43 - INFO - root -   lr after epoch 20: 0.0\n",
      "01/26/2022 21:14:43 - INFO - root -   train_loss after epoch 20: 0.11332674418867865\n",
      "01/26/2022 21:14:43 - INFO - root -   \n",
      "\n",
      "01/26/2022 21:14:43 - INFO - root -   Running evaluation\n",
      "01/26/2022 21:14:43 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 21:14:43 - INFO - root -     Batch size = 32\n",
      "01/26/2022 21:14:50 - INFO - transformers.configuration_utils -   Configuration saved in transformer-EV-topic-classification/transformers/models/output/model_out/config.json\n",
      "01/26/2022 21:14:50 - INFO - transformers.modeling_utils -   Model weights saved in transformer-EV-topic-classification/transformers/models/output/model_out/pytorch_model.bin\n",
      "01/26/2022 21:14:51 - INFO - root -   ---PROGRESS-STATUS---: Tokenizing input texts...\n",
      "01/26/2022 21:14:51 - INFO - root -   Writing example 0 of 1066\n",
      "01/26/2022 21:14:51 - INFO - root -   ---PROGRESS-STATUS---: Tokenizing input texts...DONE\n",
      "01/26/2022 21:14:51 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 1/34\n",
      "01/26/2022 21:14:51 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 2/34\n",
      "01/26/2022 21:14:51 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 3/34\n",
      "01/26/2022 21:14:51 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 4/34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/26/2022 21:14:51 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 5/34\n",
      "01/26/2022 21:14:52 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 6/34\n",
      "01/26/2022 21:14:52 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 7/34\n",
      "01/26/2022 21:14:52 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 8/34\n",
      "01/26/2022 21:14:52 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 9/34\n",
      "01/26/2022 21:14:53 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 10/34\n",
      "01/26/2022 21:14:53 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 11/34\n",
      "01/26/2022 21:14:53 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 12/34\n",
      "01/26/2022 21:14:53 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 13/34\n",
      "01/26/2022 21:14:53 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 14/34\n",
      "01/26/2022 21:14:54 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 15/34\n",
      "01/26/2022 21:14:54 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 16/34\n",
      "01/26/2022 21:14:54 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 17/34\n",
      "01/26/2022 21:14:54 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 18/34\n",
      "01/26/2022 21:14:54 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 19/34\n",
      "01/26/2022 21:14:55 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 20/34\n",
      "01/26/2022 21:14:55 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 21/34\n",
      "01/26/2022 21:14:55 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 22/34\n",
      "01/26/2022 21:14:55 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 23/34\n",
      "01/26/2022 21:14:55 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 24/34\n",
      "01/26/2022 21:14:56 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 25/34\n",
      "01/26/2022 21:14:56 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 26/34\n",
      "01/26/2022 21:14:56 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 27/34\n",
      "01/26/2022 21:14:56 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 28/34\n",
      "01/26/2022 21:14:57 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 29/34\n",
      "01/26/2022 21:14:57 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 30/34\n",
      "01/26/2022 21:14:57 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 31/34\n",
      "01/26/2022 21:14:57 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 32/34\n",
      "01/26/2022 21:14:57 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 33/34\n",
      "01/26/2022 21:14:58 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 34/34\n",
      "01/26/2022 21:14:58 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch...DONE\n",
      "Training time : 4575.410409212112\n",
      "Prediction time : 7.1758341789245605\n"
     ]
    }
   ],
   "source": [
    "!python transformer-EV-topic-classification/transformers/code/train_bert.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "transformer-EV-topic-classification/transformers/code/bert_inference.py:30: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n",
      "01/26/2022 21:15:54 - INFO - root -   {'run_text': 'multilabel topic classification with freezable layers', 'train_size': -1, 'val_size': -1, 'log_path': PosixPath('transformer-EV-topic-classification/transformers/logs'), 'full_data_dir': PosixPath('transformer-EV-topic-classification/anonymized_data'), 'data_dir': PosixPath('transformer-EV-topic-classification/anonymized_data'), 'task_name': 'topics', 'no_cuda': False, 'bert_model': None, 'output_dir': PosixPath('transformer-EV-topic-classification/transformers/models/output'), 'max_seq_length': 512, 'do_train': True, 'do_eval': True, 'do_lower_case': True, 'train_batch_size': 8, 'eval_batch_size': 8, 'learning_rate': 0.0001, 'num_train_epochs': 20, 'warmup_proportion': 0.0, 'local_rank': -1, 'seed': 42, 'gradient_accumulation_steps': 1, 'optimize_on_cpu': False, 'fp16': False, 'fp16_opt_level': 'O1', 'weight_decay': 0.01, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'max_steps': -1, 'warmup_steps': 500, 'logging_steps': 50, 'eval_all_checkpoints': True, 'overwrite_output_dir': True, 'overwrite_cache': False, 'loss_scale': 128, 'model_name': 'bert-base-cased', 'model_type': 'bert'}\n",
      "01/26/2022 21:15:55 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/azure12ev22/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
      "01/26/2022 21:15:55 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "01/26/2022 21:15:55 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/azure12ev22/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "01/26/2022 21:15:55 - INFO - root -   Loading features from cached file transformer-EV-topic-classification/anonymized_data/cache/cached_bert_train_multi_label_512_train_final.csv\n",
      "01/26/2022 21:15:57 - INFO - root -   Loading features from cached file transformer-EV-topic-classification/anonymized_data/cache/cached_bert_dev_multi_label_512_valid_final.csv\n",
      "01/26/2022 21:15:57 - INFO - root -   Loading features from cached file transformer-EV-topic-classification/anonymized_data/cache/cached_bert_test_multi_label_512_test\n",
      "8\n",
      "cuda:0\n",
      "01/26/2022 21:15:57 - INFO - transformers.configuration_utils -   loading configuration file transformer-EV-topic-classification/transformers/models/output/model_out/config.json\n",
      "01/26/2022 21:15:57 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "01/26/2022 21:15:57 - INFO - transformers.tokenization_utils_base -   Model name 'transformer-EV-topic-classification/transformers/models/output/model_out/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'transformer-EV-topic-classification/transformers/models/output/model_out/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "01/26/2022 21:15:57 - INFO - transformers.tokenization_utils_base -   Didn't find file transformer-EV-topic-classification/transformers/models/output/model_out/tokenizer.json. We won't load it.\n",
      "01/26/2022 21:15:57 - INFO - transformers.tokenization_utils_base -   loading file transformer-EV-topic-classification/transformers/models/output/model_out/vocab.txt\n",
      "01/26/2022 21:15:57 - INFO - transformers.tokenization_utils_base -   loading file transformer-EV-topic-classification/transformers/models/output/model_out/added_tokens.json\n",
      "01/26/2022 21:15:57 - INFO - transformers.tokenization_utils_base -   loading file transformer-EV-topic-classification/transformers/models/output/model_out/special_tokens_map.json\n",
      "01/26/2022 21:15:57 - INFO - transformers.tokenization_utils_base -   loading file transformer-EV-topic-classification/transformers/models/output/model_out/tokenizer_config.json\n",
      "01/26/2022 21:15:57 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "01/26/2022 21:15:57 - INFO - transformers.configuration_utils -   loading configuration file transformer-EV-topic-classification/transformers/models/output/model_out/config.json\n",
      "01/26/2022 21:15:57 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "01/26/2022 21:15:57 - INFO - transformers.modeling_utils -   loading weights file transformer-EV-topic-classification/transformers/models/output/model_out/pytorch_model.bin\n",
      "01/26/2022 21:16:00 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing BertForMultiLabelSequenceClassification.\n",
      "\n",
      "01/26/2022 21:16:00 - INFO - transformers.modeling_utils -   All the weights of BertForMultiLabelSequenceClassification were initialized from the model checkpoint at transformer-EV-topic-classification/transformers/models/output/model_out/.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForMultiLabelSequenceClassification for predictions without further training.\n",
      "01/26/2022 21:16:03 - INFO - root -   Writing example 0 of 1066\n"
     ]
    }
   ],
   "source": [
    "!python transformer-EV-topic-classification/transformers/code/bert_inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

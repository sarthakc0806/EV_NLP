{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'transformer-EV-topic-classification'...\n",
      "remote: Enumerating objects: 121, done.\u001b[K\n",
      "remote: Counting objects: 100% (121/121), done.\u001b[K\n",
      "remote: Compressing objects: 100% (109/109), done.\u001b[K\n",
      "remote: Total 121 (delta 39), reused 27 (delta 4), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (121/121), 1.60 MiB | 8.56 MiB/s, done.\n",
      "Resolving deltas: 100% (39/39), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/asensio-lab/transformer-EV-topic-classification.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "transformer-EV-topic-classification/transformers/code/train_xlnet.py:30: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n",
      "01/26/2022 21:24:21 - INFO - root -   {'run_text': 'multilabel topic classification with freezable layers', 'train_size': -1, 'val_size': -1, 'log_path': PosixPath('transformer-EV-topic-classification/transformers/logs'), 'full_data_dir': PosixPath('transformer-EV-topic-classification/anonymized_data'), 'data_dir': PosixPath('transformer-EV-topic-classification/anonymized_data'), 'task_name': 'topics', 'no_cuda': False, 'bert_model': None, 'output_dir': PosixPath('transformer-EV-topic-classification/transformers/models/output'), 'max_seq_length': 512, 'do_train': True, 'do_eval': True, 'do_lower_case': True, 'train_batch_size': 8, 'eval_batch_size': 8, 'learning_rate': 0.0001, 'num_train_epochs': 20, 'warmup_proportion': 0.0, 'local_rank': -1, 'seed': 42, 'gradient_accumulation_steps': 1, 'optimize_on_cpu': False, 'fp16': False, 'fp16_opt_level': 'O1', 'weight_decay': 0.01, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'max_steps': -1, 'warmup_steps': 500, 'logging_steps': 50, 'eval_all_checkpoints': True, 'overwrite_output_dir': True, 'overwrite_cache': False, 'loss_scale': 128, 'model_name': 'xlnet-base-cased', 'model_type': 'xlnet'}\n",
      "01/26/2022 21:24:21 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-config.json from cache at /home/azure12ev22/.cache/torch/transformers/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.69e5e35e0b798cab5e473f253752f8bf4d280ee37682281a23eed80f6e2d09c6\n",
      "01/26/2022 21:24:21 - INFO - transformers.configuration_utils -   Model config XLNetConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLNetLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_type\": \"bi\",\n",
      "  \"bi_data\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"clamp_len\": -1,\n",
      "  \"d_head\": 64,\n",
      "  \"d_inner\": 3072,\n",
      "  \"d_model\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"end_n_top\": 5,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ff_activation\": \"gelu\",\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mem_len\": null,\n",
      "  \"model_type\": \"xlnet\",\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"pad_token_id\": 5,\n",
      "  \"reuse_len\": null,\n",
      "  \"same_length\": false,\n",
      "  \"start_n_top\": 5,\n",
      "  \"summary_activation\": \"tanh\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"last\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 250\n",
      "    }\n",
      "  },\n",
      "  \"untie_r\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "01/26/2022 21:24:21 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-spiece.model from cache at /home/azure12ev22/.cache/torch/transformers/dad589d582573df0293448af5109cb6981ca77239ed314e15ca63b7b8a318ddd.8b10bd978b5d01c21303cc761fc9ecd464419b3bf921864a355ba807cfbfafa8\n",
      "01/26/2022 21:24:26 - INFO - root -   Writing example 0 of 8521\n",
      "01/26/2022 21:24:27 - INFO - root -   Saving features into cached file transformer-EV-topic-classification/anonymized_data/cache/cached_xlnet_train_multi_label_512_train_final.csv\n",
      "01/26/2022 21:24:32 - INFO - root -   Writing example 0 of 1065\n",
      "01/26/2022 21:24:32 - INFO - root -   Saving features into cached file transformer-EV-topic-classification/anonymized_data/cache/cached_xlnet_dev_multi_label_512_valid_final.csv\n",
      "01/26/2022 21:24:32 - INFO - root -   Writing example 0 of 14\n",
      "01/26/2022 21:24:32 - INFO - root -   Saving features into cached file transformer-EV-topic-classification/anonymized_data/cache/cached_xlnet_test_multi_label_512_test\n",
      "8\n",
      "cuda:0\n",
      "01/26/2022 21:24:32 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-config.json from cache at /home/azure12ev22/.cache/torch/transformers/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.69e5e35e0b798cab5e473f253752f8bf4d280ee37682281a23eed80f6e2d09c6\n",
      "01/26/2022 21:24:32 - INFO - transformers.configuration_utils -   Model config XLNetConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLNetLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_type\": \"bi\",\n",
      "  \"bi_data\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"clamp_len\": -1,\n",
      "  \"d_head\": 64,\n",
      "  \"d_inner\": 3072,\n",
      "  \"d_model\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"end_n_top\": 5,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ff_activation\": \"gelu\",\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mem_len\": null,\n",
      "  \"model_type\": \"xlnet\",\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"pad_token_id\": 5,\n",
      "  \"reuse_len\": null,\n",
      "  \"same_length\": false,\n",
      "  \"start_n_top\": 5,\n",
      "  \"summary_activation\": \"tanh\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"last\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 250\n",
      "    }\n",
      "  },\n",
      "  \"untie_r\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "01/26/2022 21:24:33 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/xlnet-base-cased-pytorch_model.bin from cache at /home/azure12ev22/.cache/torch/transformers/33d6135fea0154c088449506a4c5f9553cb59b6fd040138417a7033af64bb8f9.7eac4fe898a021204e63c88c00ea68c60443c57f94b4bc3c02adbde6465745ac\n",
      "01/26/2022 21:24:39 - WARNING - transformers.modeling_utils -   Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "01/26/2022 21:24:39 - WARNING - transformers.modeling_utils -   Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "01/26/2022 21:24:43 - INFO - root -   ***** Running training *****\n",
      "01/26/2022 21:24:43 - INFO - root -     Num examples = 8521\n",
      "01/26/2022 21:24:43 - INFO - root -     Num Epochs = 20\n",
      "01/26/2022 21:24:43 - INFO - root -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "01/26/2022 21:24:43 - INFO - root -     Gradient Accumulation steps = 1\n",
      "01/26/2022 21:24:43 - INFO - root -     Total optimization steps = 10660\n",
      "/home/azure12ev22/anaconda3/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/azure12ev22/anaconda3/lib/python3.8/site-packages/pytorch_lamb/lamb.py:96: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "01/26/2022 21:31:52 - INFO - root -   Running evaluation         \n",
      "01/26/2022 21:31:52 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 21:31:52 - INFO - root -     Batch size = 32\n",
      "01/26/2022 21:32:18 - INFO - root -   eval_loss after epoch 1: 0.36697228866464954: \n",
      "01/26/2022 21:32:18 - INFO - root -   eval_accuracy after epoch 1: 0.0: \n",
      "01/26/2022 21:32:18 - INFO - root -   eval_accuracy_thresh after epoch 1: 0.8534038066864014: \n",
      "01/26/2022 21:32:18 - INFO - root -   eval_roc_auc after epoch 1: 0.8384304191383837: \n",
      "01/26/2022 21:32:18 - INFO - root -   eval_fbeta after epoch 1: 0.593654990196228: \n",
      "/home/azure12ev22/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "01/26/2022 21:32:18 - INFO - root -   lr after epoch 1: 9.999739698632535e-05\n",
      "01/26/2022 21:32:18 - INFO - root -   train_loss after epoch 1: 0.49889256515825\n",
      "01/26/2022 21:32:18 - INFO - root -   \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/26/2022 21:39:25 - INFO - root -   Running evaluation         \n",
      "01/26/2022 21:39:25 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 21:39:25 - INFO - root -     Batch size = 32\n",
      "01/26/2022 21:39:50 - INFO - root -   eval_loss after epoch 2: 0.267999991774559: \n",
      "01/26/2022 21:39:50 - INFO - root -   eval_accuracy after epoch 2: 0.0: \n",
      "01/26/2022 21:39:50 - INFO - root -   eval_accuracy_thresh after epoch 2: 0.8956573009490967: \n",
      "01/26/2022 21:39:50 - INFO - root -   eval_roc_auc after epoch 2: 0.9222940992144532: \n",
      "01/26/2022 21:39:50 - INFO - root -   eval_fbeta after epoch 2: 0.7659921050071716: \n",
      "01/26/2022 21:39:50 - INFO - root -   lr after epoch 2: 9.923620574858906e-05\n",
      "01/26/2022 21:39:50 - INFO - root -   train_loss after epoch 2: 0.33314084017030743\n",
      "01/26/2022 21:39:50 - INFO - root -   \n",
      "\n",
      "01/26/2022 21:46:58 - INFO - root -   Running evaluation         \n",
      "01/26/2022 21:46:58 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 21:46:58 - INFO - root -     Batch size = 32\n",
      "01/26/2022 21:47:23 - INFO - root -   eval_loss after epoch 3: 0.23550829510478413: \n",
      "01/26/2022 21:47:23 - INFO - root -   eval_accuracy after epoch 3: 0.0: \n",
      "01/26/2022 21:47:23 - INFO - root -   eval_accuracy_thresh after epoch 3: 0.905985951423645: \n",
      "01/26/2022 21:47:23 - INFO - root -   eval_roc_auc after epoch 3: 0.940932326277459: \n",
      "01/26/2022 21:47:23 - INFO - root -   eval_fbeta after epoch 3: 0.7998030185699463: \n",
      "01/26/2022 21:47:23 - INFO - root -   lr after epoch 3: 9.714066971576419e-05\n",
      "01/26/2022 21:47:23 - INFO - root -   train_loss after epoch 3: 0.2651352017251233\n",
      "01/26/2022 21:47:23 - INFO - root -   \n",
      "\n",
      "01/26/2022 21:54:31 - INFO - root -   Running evaluation         \n",
      "01/26/2022 21:54:31 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 21:54:31 - INFO - root -     Batch size = 32\n",
      "01/26/2022 21:54:56 - INFO - root -   eval_loss after epoch 4: 0.22216276736820445: \n",
      "01/26/2022 21:54:56 - INFO - root -   eval_accuracy after epoch 4: 0.0: \n",
      "01/26/2022 21:54:56 - INFO - root -   eval_accuracy_thresh after epoch 4: 0.9103286862373352: \n",
      "01/26/2022 21:54:56 - INFO - root -   eval_roc_auc after epoch 4: 0.9491033528909636: \n",
      "01/26/2022 21:54:56 - INFO - root -   eval_fbeta after epoch 4: 0.8215344548225403: \n",
      "01/26/2022 21:54:56 - INFO - root -   lr after epoch 4: 9.376757977081594e-05\n",
      "01/26/2022 21:54:56 - INFO - root -   train_loss after epoch 4: 0.2325081579093638\n",
      "01/26/2022 21:54:56 - INFO - root -   \n",
      "\n",
      "01/26/2022 22:02:04 - INFO - root -   Running evaluation         \n",
      "01/26/2022 22:02:04 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 22:02:04 - INFO - root -     Batch size = 32\n",
      "01/26/2022 22:02:29 - INFO - root -   eval_loss after epoch 5: 0.2169580341261976: \n",
      "01/26/2022 22:02:29 - INFO - root -   eval_accuracy after epoch 5: 0.0: \n",
      "01/26/2022 22:02:29 - INFO - root -   eval_accuracy_thresh after epoch 5: 0.9117370843887329: \n",
      "01/26/2022 22:02:29 - INFO - root -   eval_roc_auc after epoch 5: 0.9522850011345587: \n",
      "01/26/2022 22:02:29 - INFO - root -   eval_fbeta after epoch 5: 0.8269115686416626: \n",
      "01/26/2022 22:02:29 - INFO - root -   lr after epoch 5: 8.920834963953769e-05\n",
      "01/26/2022 22:02:29 - INFO - root -   train_loss after epoch 5: 0.21149781635677614\n",
      "01/26/2022 22:02:29 - INFO - root -   \n",
      "\n",
      "01/26/2022 22:09:35 - INFO - root -   Running evaluation         \n",
      "01/26/2022 22:09:35 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 22:09:35 - INFO - root -     Batch size = 32\n",
      "01/26/2022 22:10:01 - INFO - root -   eval_loss after epoch 6: 0.21000861990101197: \n",
      "01/26/2022 22:10:01 - INFO - root -   eval_accuracy after epoch 6: 0.0: \n",
      "01/26/2022 22:10:01 - INFO - root -   eval_accuracy_thresh after epoch 6: 0.9157277345657349: \n",
      "01/26/2022 22:10:01 - INFO - root -   eval_roc_auc after epoch 6: 0.9551884339848942: \n",
      "01/26/2022 22:10:01 - INFO - root -   eval_fbeta after epoch 6: 0.8234464526176453: \n",
      "01/26/2022 22:10:01 - INFO - root -   lr after epoch 6: 8.358653849759678e-05\n",
      "01/26/2022 22:10:01 - INFO - root -   train_loss after epoch 6: 0.19677406244720796\n",
      "01/26/2022 22:10:01 - INFO - root -   \n",
      "\n",
      "01/26/2022 22:17:07 - INFO - root -   Running evaluation         \n",
      "01/26/2022 22:17:07 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 22:17:07 - INFO - root -     Batch size = 32\n",
      "01/26/2022 22:17:32 - INFO - root -   eval_loss after epoch 7: 0.2114674431436202: \n",
      "01/26/2022 22:17:32 - INFO - root -   eval_accuracy after epoch 7: 0.0: \n",
      "01/26/2022 22:17:32 - INFO - root -   eval_accuracy_thresh after epoch 7: 0.9150235056877136: \n",
      "01/26/2022 22:17:32 - INFO - root -   eval_roc_auc after epoch 7: 0.9549633375474083: \n",
      "01/26/2022 22:17:32 - INFO - root -   eval_fbeta after epoch 7: 0.8211874961853027: \n",
      "01/26/2022 22:17:32 - INFO - root -   lr after epoch 7: 7.705450240741043e-05\n",
      "01/26/2022 22:17:32 - INFO - root -   train_loss after epoch 7: 0.18505085734947538\n",
      "01/26/2022 22:17:32 - INFO - root -   \n",
      "\n",
      "01/26/2022 22:24:38 - INFO - root -   Running evaluation         \n",
      "01/26/2022 22:24:38 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 22:24:38 - INFO - root -     Batch size = 32\n",
      "01/26/2022 22:25:04 - INFO - root -   eval_loss after epoch 8: 0.21089114818502874: \n",
      "01/26/2022 22:25:04 - INFO - root -   eval_accuracy after epoch 8: 0.0: \n",
      "01/26/2022 22:25:04 - INFO - root -   eval_accuracy_thresh after epoch 8: 0.9143192768096924: \n",
      "01/26/2022 22:25:04 - INFO - root -   eval_roc_auc after epoch 8: 0.9557421850519197: \n",
      "01/26/2022 22:25:04 - INFO - root -   eval_fbeta after epoch 8: 0.8351179957389832: \n",
      "01/26/2022 22:25:04 - INFO - root -   lr after epoch 8: 6.978926533387911e-05\n",
      "01/26/2022 22:25:04 - INFO - root -   train_loss after epoch 8: 0.1728935831315522\n",
      "01/26/2022 22:25:04 - INFO - root -   \n",
      "\n",
      "01/26/2022 22:32:11 - INFO - root -   Running evaluation         \n",
      "01/26/2022 22:32:11 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 22:32:11 - INFO - root -     Batch size = 32\n",
      "01/26/2022 22:32:37 - INFO - root -   eval_loss after epoch 9: 0.2090087099110379: \n",
      "01/26/2022 22:32:37 - INFO - root -   eval_accuracy after epoch 9: 0.0: \n",
      "01/26/2022 22:32:37 - INFO - root -   eval_accuracy_thresh after epoch 9: 0.9170188307762146: \n",
      "01/26/2022 22:32:37 - INFO - root -   eval_roc_auc after epoch 9: 0.9572434330664419: \n",
      "01/26/2022 22:32:37 - INFO - root -   eval_fbeta after epoch 9: 0.8306470513343811: \n",
      "01/26/2022 22:32:37 - INFO - root -   lr after epoch 9: 6.198772163810884e-05\n",
      "01/26/2022 22:32:37 - INFO - root -   train_loss after epoch 9: 0.16449581605240732\n",
      "01/26/2022 22:32:37 - INFO - root -   \n",
      "\n",
      "01/26/2022 22:39:43 - INFO - root -   Running evaluation          \n",
      "01/26/2022 22:39:43 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 22:39:43 - INFO - root -     Batch size = 32\n",
      "01/26/2022 22:40:08 - INFO - root -   eval_loss after epoch 10: 0.21081747509100857: \n",
      "01/26/2022 22:40:08 - INFO - root -   eval_accuracy after epoch 10: 0.0: \n",
      "01/26/2022 22:40:08 - INFO - root -   eval_accuracy_thresh after epoch 10: 0.9164319634437561: \n",
      "01/26/2022 22:40:08 - INFO - root -   eval_roc_auc after epoch 10: 0.9567089154699774: \n",
      "01/26/2022 22:40:08 - INFO - root -   eval_fbeta after epoch 10: 0.8313897848129272: \n",
      "01/26/2022 22:40:08 - INFO - root -   lr after epoch 10: 5.38613000657926e-05\n",
      "01/26/2022 22:40:08 - INFO - root -   train_loss after epoch 10: 0.1560760039004443\n",
      "01/26/2022 22:40:08 - INFO - root -   \n",
      "\n",
      "01/26/2022 22:47:14 - INFO - root -   Running evaluation          \n",
      "01/26/2022 22:47:14 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 22:47:14 - INFO - root -     Batch size = 32\n",
      "01/26/2022 22:47:40 - INFO - root -   eval_loss after epoch 11: 0.21238470603437984: \n",
      "01/26/2022 22:47:40 - INFO - root -   eval_accuracy after epoch 11: 0.0: \n",
      "01/26/2022 22:47:40 - INFO - root -   eval_accuracy_thresh after epoch 11: 0.9158450961112976: \n",
      "01/26/2022 22:47:40 - INFO - root -   eval_roc_auc after epoch 11: 0.9567463883216095: \n",
      "01/26/2022 22:47:40 - INFO - root -   eval_fbeta after epoch 11: 0.8318527340888977: \n",
      "01/26/2022 22:47:40 - INFO - root -   lr after epoch 11: 4.5630233840893374e-05\n",
      "01/26/2022 22:47:40 - INFO - root -   train_loss after epoch 11: 0.14764309215305224\n",
      "01/26/2022 22:47:40 - INFO - root -   \n",
      "\n",
      "01/26/2022 22:54:46 - INFO - root -   Running evaluation          \n",
      "01/26/2022 22:54:46 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 22:54:46 - INFO - root -     Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/26/2022 22:55:11 - INFO - root -   eval_loss after epoch 12: 0.21517563786576777: \n",
      "01/26/2022 22:55:11 - INFO - root -   eval_accuracy after epoch 12: 0.0: \n",
      "01/26/2022 22:55:11 - INFO - root -   eval_accuracy_thresh after epoch 12: 0.9144366383552551: \n",
      "01/26/2022 22:55:11 - INFO - root -   eval_roc_auc after epoch 12: 0.9564332933536474: \n",
      "01/26/2022 22:55:11 - INFO - root -   eval_fbeta after epoch 12: 0.8314680457115173: \n",
      "01/26/2022 22:55:11 - INFO - root -   lr after epoch 12: 3.751759215016583e-05\n",
      "01/26/2022 22:55:11 - INFO - root -   train_loss after epoch 12: 0.1416281065702662\n",
      "01/26/2022 22:55:11 - INFO - root -   \n",
      "\n",
      "01/26/2022 23:02:17 - INFO - root -   Running evaluation          \n",
      "01/26/2022 23:02:17 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 23:02:17 - INFO - root -     Batch size = 32\n",
      "01/26/2022 23:02:43 - INFO - root -   eval_loss after epoch 13: 0.21527518200523713: \n",
      "01/26/2022 23:02:43 - INFO - root -   eval_accuracy after epoch 13: 0.0: \n",
      "01/26/2022 23:02:43 - INFO - root -   eval_accuracy_thresh after epoch 13: 0.9163146018981934: \n",
      "01/26/2022 23:02:43 - INFO - root -   eval_roc_auc after epoch 13: 0.9565793812873458: \n",
      "01/26/2022 23:02:43 - INFO - root -   eval_fbeta after epoch 13: 0.8273727893829346: \n",
      "01/26/2022 23:02:43 - INFO - root -   lr after epoch 13: 2.9743234770574034e-05\n",
      "01/26/2022 23:02:43 - INFO - root -   train_loss after epoch 13: 0.13615727014350398\n",
      "01/26/2022 23:02:43 - INFO - root -   \n",
      "\n",
      "01/26/2022 23:09:48 - INFO - root -   Running evaluation          \n",
      "01/26/2022 23:09:48 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 23:09:48 - INFO - root -     Batch size = 32\n",
      "01/26/2022 23:10:14 - INFO - root -   eval_loss after epoch 14: 0.21907038811375112: \n",
      "01/26/2022 23:10:14 - INFO - root -   eval_accuracy after epoch 14: 0.0: \n",
      "01/26/2022 23:10:14 - INFO - root -   eval_accuracy_thresh after epoch 14: 0.9134976863861084: \n",
      "01/26/2022 23:10:14 - INFO - root -   eval_roc_auc after epoch 14: 0.9559038326471956: \n",
      "01/26/2022 23:10:14 - INFO - root -   eval_fbeta after epoch 14: 0.8318945169448853: \n",
      "01/26/2022 23:10:14 - INFO - root -   lr after epoch 14: 2.2517853674557698e-05\n",
      "01/26/2022 23:10:14 - INFO - root -   train_loss after epoch 14: 0.1311733364206206\n",
      "01/26/2022 23:10:14 - INFO - root -   \n",
      "\n",
      "01/26/2022 23:17:19 - INFO - root -   Running evaluation          \n",
      "01/26/2022 23:17:19 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 23:17:19 - INFO - root -     Batch size = 32\n",
      "01/26/2022 23:17:44 - INFO - root -   eval_loss after epoch 15: 0.21869353161138647: \n",
      "01/26/2022 23:17:44 - INFO - root -   eval_accuracy after epoch 15: 0.0: \n",
      "01/26/2022 23:17:44 - INFO - root -   eval_accuracy_thresh after epoch 15: 0.9144366383552551: \n",
      "01/26/2022 23:17:44 - INFO - root -   eval_roc_auc after epoch 15: 0.9560633191783633: \n",
      "01/26/2022 23:17:44 - INFO - root -   eval_fbeta after epoch 15: 0.8317126035690308: \n",
      "01/26/2022 23:17:44 - INFO - root -   lr after epoch 15: 1.6037263090922948e-05\n",
      "01/26/2022 23:17:44 - INFO - root -   train_loss after epoch 15: 0.12680959745570597\n",
      "01/26/2022 23:17:44 - INFO - root -   \n",
      "\n",
      "01/26/2022 23:24:49 - INFO - root -   Running evaluation          \n",
      "01/26/2022 23:24:49 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 23:24:49 - INFO - root -     Batch size = 32\n",
      "01/26/2022 23:25:15 - INFO - root -   eval_loss after epoch 16: 0.22144209374399745: \n",
      "01/26/2022 23:25:15 - INFO - root -   eval_accuracy after epoch 16: 0.0: \n",
      "01/26/2022 23:25:15 - INFO - root -   eval_accuracy_thresh after epoch 16: 0.9134976863861084: \n",
      "01/26/2022 23:25:15 - INFO - root -   eval_roc_auc after epoch 16: 0.955412752439301: \n",
      "01/26/2022 23:25:15 - INFO - root -   eval_fbeta after epoch 16: 0.8275927305221558: \n",
      "01/26/2022 23:25:15 - INFO - root -   lr after epoch 16: 1.0477092765766162e-05\n",
      "01/26/2022 23:25:15 - INFO - root -   train_loss after epoch 16: 0.12411978608783593\n",
      "01/26/2022 23:25:15 - INFO - root -   \n",
      "\n",
      "01/26/2022 23:32:20 - INFO - root -   Running evaluation          \n",
      "01/26/2022 23:32:20 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 23:32:20 - INFO - root -     Batch size = 32\n",
      "01/26/2022 23:32:45 - INFO - root -   eval_loss after epoch 17: 0.22123945153811397: \n",
      "01/26/2022 23:32:45 - INFO - root -   eval_accuracy after epoch 17: 0.0: \n",
      "01/26/2022 23:32:45 - INFO - root -   eval_accuracy_thresh after epoch 17: 0.9140845537185669: \n",
      "01/26/2022 23:32:45 - INFO - root -   eval_roc_auc after epoch 17: 0.9553361643273147: \n",
      "01/26/2022 23:32:45 - INFO - root -   eval_fbeta after epoch 17: 0.8277592062950134: \n",
      "01/26/2022 23:32:45 - INFO - root -   lr after epoch 17: 5.988028240760935e-06\n",
      "01/26/2022 23:32:45 - INFO - root -   train_loss after epoch 17: 0.12322489541254616\n",
      "01/26/2022 23:32:45 - INFO - root -   \n",
      "\n",
      "01/26/2022 23:39:51 - INFO - root -   Running evaluation          \n",
      "01/26/2022 23:39:51 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 23:39:51 - INFO - root -     Batch size = 32\n",
      "01/26/2022 23:40:16 - INFO - root -   eval_loss after epoch 18: 0.22133075139101813: \n",
      "01/26/2022 23:40:16 - INFO - root -   eval_accuracy after epoch 18: 0.0: \n",
      "01/26/2022 23:40:16 - INFO - root -   eval_accuracy_thresh after epoch 18: 0.9137324094772339: \n",
      "01/26/2022 23:40:16 - INFO - root -   eval_roc_auc after epoch 18: 0.9554963423989972: \n",
      "01/26/2022 23:40:16 - INFO - root -   eval_fbeta after epoch 18: 0.8257168531417847: \n",
      "01/26/2022 23:40:16 - INFO - root -   lr after epoch 18: 2.6917271414835297e-06\n",
      "01/26/2022 23:40:16 - INFO - root -   train_loss after epoch 18: 0.12116854769958788\n",
      "01/26/2022 23:40:16 - INFO - root -   \n",
      "\n",
      "01/26/2022 23:47:23 - INFO - root -   Running evaluation          \n",
      "01/26/2022 23:47:23 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 23:47:23 - INFO - root -     Batch size = 32\n",
      "01/26/2022 23:47:48 - INFO - root -   eval_loss after epoch 19: 0.22150662892005024: \n",
      "01/26/2022 23:47:48 - INFO - root -   eval_accuracy after epoch 19: 0.0: \n",
      "01/26/2022 23:47:48 - INFO - root -   eval_accuracy_thresh after epoch 19: 0.9133803248405457: \n",
      "01/26/2022 23:47:48 - INFO - root -   eval_roc_auc after epoch 19: 0.9555596183560785: \n",
      "01/26/2022 23:47:48 - INFO - root -   eval_fbeta after epoch 19: 0.8275152444839478: \n",
      "01/26/2022 23:47:48 - INFO - root -   lr after epoch 19: 6.775221479809302e-07\n",
      "01/26/2022 23:47:48 - INFO - root -   train_loss after epoch 19: 0.12036572889499995\n",
      "01/26/2022 23:47:48 - INFO - root -   \n",
      "\n",
      "01/26/2022 23:54:55 - INFO - root -   Running evaluation          \n",
      "01/26/2022 23:54:55 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 23:54:55 - INFO - root -     Batch size = 32\n",
      "01/26/2022 23:55:20 - INFO - root -   eval_loss after epoch 20: 0.22160098657888525: \n",
      "01/26/2022 23:55:20 - INFO - root -   eval_accuracy after epoch 20: 0.0: \n",
      "01/26/2022 23:55:20 - INFO - root -   eval_accuracy_thresh after epoch 20: 0.9134976863861084: \n",
      "01/26/2022 23:55:20 - INFO - root -   eval_roc_auc after epoch 20: 0.9555097409964667: \n",
      "01/26/2022 23:55:20 - INFO - root -   eval_fbeta after epoch 20: 0.8281779289245605: \n",
      "01/26/2022 23:55:20 - INFO - root -   lr after epoch 20: 0.0\n",
      "01/26/2022 23:55:20 - INFO - root -   train_loss after epoch 20: 0.12001977305978145\n",
      "01/26/2022 23:55:20 - INFO - root -   \n",
      "\n",
      "01/26/2022 23:55:20 - INFO - root -   Running evaluation\n",
      "01/26/2022 23:55:20 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 23:55:20 - INFO - root -     Batch size = 32\n",
      "01/26/2022 23:55:46 - INFO - transformers.configuration_utils -   Configuration saved in transformer-EV-topic-classification/transformers/models/output/model_out/config.json\n",
      "01/26/2022 23:55:46 - INFO - transformers.modeling_utils -   Model weights saved in transformer-EV-topic-classification/transformers/models/output/model_out/pytorch_model.bin\n",
      "01/26/2022 23:55:46 - INFO - root -   ---PROGRESS-STATUS---: Tokenizing input texts...\n",
      "01/26/2022 23:55:46 - INFO - root -   Writing example 0 of 1066\n",
      "01/26/2022 23:55:46 - INFO - root -   ---PROGRESS-STATUS---: Tokenizing input texts...DONE\n",
      "01/26/2022 23:55:46 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 1/34\n",
      "01/26/2022 23:55:47 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 2/34\n",
      "01/26/2022 23:55:48 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 3/34\n",
      "01/26/2022 23:55:49 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 4/34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/26/2022 23:55:49 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 5/34\n",
      "01/26/2022 23:55:50 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 6/34\n",
      "01/26/2022 23:55:51 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 7/34\n",
      "01/26/2022 23:55:52 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 8/34\n",
      "01/26/2022 23:55:52 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 9/34\n",
      "01/26/2022 23:55:53 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 10/34\n",
      "01/26/2022 23:55:54 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 11/34\n",
      "01/26/2022 23:55:55 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 12/34\n",
      "01/26/2022 23:55:56 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 13/34\n",
      "01/26/2022 23:55:56 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 14/34\n",
      "01/26/2022 23:55:57 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 15/34\n",
      "01/26/2022 23:55:58 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 16/34\n",
      "01/26/2022 23:55:59 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 17/34\n",
      "01/26/2022 23:55:59 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 18/34\n",
      "01/26/2022 23:56:00 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 19/34\n",
      "01/26/2022 23:56:01 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 20/34\n",
      "01/26/2022 23:56:02 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 21/34\n",
      "01/26/2022 23:56:02 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 22/34\n",
      "01/26/2022 23:56:03 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 23/34\n",
      "01/26/2022 23:56:04 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 24/34\n",
      "01/26/2022 23:56:05 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 25/34\n",
      "01/26/2022 23:56:05 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 26/34\n",
      "01/26/2022 23:56:06 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 27/34\n",
      "01/26/2022 23:56:07 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 28/34\n",
      "01/26/2022 23:56:08 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 29/34\n",
      "01/26/2022 23:56:08 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 30/34\n",
      "01/26/2022 23:56:09 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 31/34\n",
      "01/26/2022 23:56:10 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 32/34\n",
      "01/26/2022 23:56:11 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 33/34\n",
      "01/26/2022 23:56:11 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 34/34\n",
      "01/26/2022 23:56:12 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch...DONE\n",
      "Training time : 9037.661890029907\n",
      "Prediction time : 25.429837226867676\n"
     ]
    }
   ],
   "source": [
    "!python transformer-EV-topic-classification/transformers/code/train_xlnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "transformer-EV-topic-classification/transformers/code/xlnet_inference.py:30: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n",
      "01/26/2022 23:56:15 - INFO - root -   {'run_text': 'multilabel topic classification with freezable layers', 'train_size': -1, 'val_size': -1, 'log_path': PosixPath('transformer-EV-topic-classification/transformers/logs'), 'full_data_dir': PosixPath('transformer-EV-topic-classification/anonymized_data'), 'data_dir': PosixPath('transformer-EV-topic-classification/anonymized_data'), 'task_name': 'topics', 'no_cuda': False, 'bert_model': None, 'output_dir': PosixPath('transformer-EV-topic-classification/transformers/models/output'), 'max_seq_length': 512, 'do_train': True, 'do_eval': True, 'do_lower_case': True, 'train_batch_size': 8, 'eval_batch_size': 8, 'learning_rate': 0.0001, 'num_train_epochs': 20, 'warmup_proportion': 0.0, 'local_rank': -1, 'seed': 42, 'gradient_accumulation_steps': 1, 'optimize_on_cpu': False, 'fp16': False, 'fp16_opt_level': 'O1', 'weight_decay': 0.01, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'max_steps': -1, 'warmup_steps': 500, 'logging_steps': 50, 'eval_all_checkpoints': True, 'overwrite_output_dir': True, 'overwrite_cache': False, 'loss_scale': 128, 'model_name': 'xlnet-base-cased', 'model_type': 'xlnet'}\n",
      "01/26/2022 23:56:15 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-config.json from cache at /home/azure12ev22/.cache/torch/transformers/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.69e5e35e0b798cab5e473f253752f8bf4d280ee37682281a23eed80f6e2d09c6\n",
      "01/26/2022 23:56:15 - INFO - transformers.configuration_utils -   Model config XLNetConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLNetLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_type\": \"bi\",\n",
      "  \"bi_data\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"clamp_len\": -1,\n",
      "  \"d_head\": 64,\n",
      "  \"d_inner\": 3072,\n",
      "  \"d_model\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"end_n_top\": 5,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ff_activation\": \"gelu\",\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mem_len\": null,\n",
      "  \"model_type\": \"xlnet\",\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"pad_token_id\": 5,\n",
      "  \"reuse_len\": null,\n",
      "  \"same_length\": false,\n",
      "  \"start_n_top\": 5,\n",
      "  \"summary_activation\": \"tanh\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"last\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 250\n",
      "    }\n",
      "  },\n",
      "  \"untie_r\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "01/26/2022 23:56:15 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-spiece.model from cache at /home/azure12ev22/.cache/torch/transformers/dad589d582573df0293448af5109cb6981ca77239ed314e15ca63b7b8a318ddd.8b10bd978b5d01c21303cc761fc9ecd464419b3bf921864a355ba807cfbfafa8\n",
      "01/26/2022 23:56:16 - INFO - root -   Loading features from cached file transformer-EV-topic-classification/anonymized_data/cache/cached_xlnet_train_multi_label_512_train_final.csv\n",
      "01/26/2022 23:56:18 - INFO - root -   Loading features from cached file transformer-EV-topic-classification/anonymized_data/cache/cached_xlnet_dev_multi_label_512_valid_final.csv\n",
      "01/26/2022 23:56:18 - INFO - root -   Loading features from cached file transformer-EV-topic-classification/anonymized_data/cache/cached_xlnet_test_multi_label_512_test\n",
      "8\n",
      "cuda:0\n",
      "01/26/2022 23:56:18 - INFO - transformers.configuration_utils -   loading configuration file transformer-EV-topic-classification/transformers/models/output/model_out/config.json\n",
      "01/26/2022 23:56:18 - INFO - transformers.configuration_utils -   Model config XLNetConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLNetForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attn_type\": \"bi\",\n",
      "  \"bi_data\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"clamp_len\": -1,\n",
      "  \"d_head\": 64,\n",
      "  \"d_inner\": 3072,\n",
      "  \"d_model\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"end_n_top\": 5,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ff_activation\": \"gelu\",\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mem_len\": null,\n",
      "  \"model_type\": \"xlnet\",\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"pad_token_id\": 5,\n",
      "  \"reuse_len\": null,\n",
      "  \"same_length\": false,\n",
      "  \"start_n_top\": 5,\n",
      "  \"summary_activation\": \"tanh\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"last\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 250\n",
      "    }\n",
      "  },\n",
      "  \"untie_r\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "01/26/2022 23:56:18 - INFO - transformers.tokenization_utils_base -   Model name 'transformer-EV-topic-classification/transformers/models/output/model_out/' not found in model shortcut name list (xlnet-base-cased, xlnet-large-cased). Assuming 'transformer-EV-topic-classification/transformers/models/output/model_out/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "01/26/2022 23:56:18 - INFO - transformers.tokenization_utils_base -   Didn't find file transformer-EV-topic-classification/transformers/models/output/model_out/tokenizer.json. We won't load it.\n",
      "01/26/2022 23:56:18 - INFO - transformers.tokenization_utils_base -   loading file transformer-EV-topic-classification/transformers/models/output/model_out/spiece.model\n",
      "01/26/2022 23:56:18 - INFO - transformers.tokenization_utils_base -   loading file transformer-EV-topic-classification/transformers/models/output/model_out/added_tokens.json\n",
      "01/26/2022 23:56:18 - INFO - transformers.tokenization_utils_base -   loading file transformer-EV-topic-classification/transformers/models/output/model_out/special_tokens_map.json\n",
      "01/26/2022 23:56:18 - INFO - transformers.tokenization_utils_base -   loading file transformer-EV-topic-classification/transformers/models/output/model_out/tokenizer_config.json\n",
      "01/26/2022 23:56:18 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "01/26/2022 23:56:18 - INFO - transformers.configuration_utils -   loading configuration file transformer-EV-topic-classification/transformers/models/output/model_out/config.json\n",
      "01/26/2022 23:56:18 - INFO - transformers.configuration_utils -   Model config XLNetConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLNetForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attn_type\": \"bi\",\n",
      "  \"bi_data\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"clamp_len\": -1,\n",
      "  \"d_head\": 64,\n",
      "  \"d_inner\": 3072,\n",
      "  \"d_model\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"end_n_top\": 5,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ff_activation\": \"gelu\",\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mem_len\": null,\n",
      "  \"model_type\": \"xlnet\",\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"pad_token_id\": 5,\n",
      "  \"reuse_len\": null,\n",
      "  \"same_length\": false,\n",
      "  \"start_n_top\": 5,\n",
      "  \"summary_activation\": \"tanh\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"last\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 250\n",
      "    }\n",
      "  },\n",
      "  \"untie_r\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "01/26/2022 23:56:18 - INFO - transformers.modeling_utils -   loading weights file transformer-EV-topic-classification/transformers/models/output/model_out/pytorch_model.bin\n",
      "01/26/2022 23:56:21 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing XLNetForMultiLabelSequenceClassification.\n",
      "\n",
      "01/26/2022 23:56:21 - INFO - transformers.modeling_utils -   All the weights of XLNetForMultiLabelSequenceClassification were initialized from the model checkpoint at transformer-EV-topic-classification/transformers/models/output/model_out/.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use XLNetForMultiLabelSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/26/2022 23:56:24 - INFO - root -   Writing example 0 of 1066\r\n"
     ]
    }
   ],
   "source": [
    "!python transformer-EV-topic-classification/transformers/code/xlnet_inference.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'transformer-EV-topic-classification'...\n",
      "remote: Enumerating objects: 121, done.\u001b[K\n",
      "remote: Counting objects: 100% (121/121), done.\u001b[K\n",
      "remote: Compressing objects: 100% (109/109), done.\u001b[K\n",
      "remote: Total 121 (delta 39), reused 27 (delta 4), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (121/121), 1.60 MiB | 6.99 MiB/s, done.\n",
      "Resolving deltas: 100% (39/39), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/asensio-lab/transformer-EV-topic-classification.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "transformer-EV-topic-classification/transformers/code/train_bert.py:30: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n",
      "01/25/2022 18:42:52 - INFO - root -   {'run_text': 'multilabel topic classification with freezable layers', 'train_size': -1, 'val_size': -1, 'log_path': PosixPath('transformer-EV-topic-classification/transformers/logs'), 'full_data_dir': PosixPath('transformer-EV-topic-classification/anonymized_data'), 'data_dir': PosixPath('transformer-EV-topic-classification/anonymized_data'), 'task_name': 'topics', 'no_cuda': False, 'bert_model': None, 'output_dir': PosixPath('transformer-EV-topic-classification/transformers/models/output'), 'max_seq_length': 512, 'do_train': True, 'do_eval': True, 'do_lower_case': True, 'train_batch_size': 8, 'eval_batch_size': 8, 'learning_rate': 0.0001, 'num_train_epochs': 20, 'warmup_proportion': 0.0, 'local_rank': -1, 'seed': 42, 'gradient_accumulation_steps': 1, 'optimize_on_cpu': False, 'fp16': False, 'fp16_opt_level': 'O1', 'weight_decay': 0.01, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'max_steps': -1, 'warmup_steps': 500, 'logging_steps': 50, 'eval_all_checkpoints': True, 'overwrite_output_dir': True, 'overwrite_cache': False, 'loss_scale': 128, 'model_name': 'bert-base-cased', 'model_type': 'bert'}\n",
      "01/25/2022 18:42:53 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/azure12ev22/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
      "01/25/2022 18:42:53 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "01/25/2022 18:42:53 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/azure12ev22/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "01/25/2022 18:42:57 - INFO - root -   Writing example 0 of 8521\n",
      "01/25/2022 18:42:58 - INFO - root -   Saving features into cached file transformer-EV-topic-classification/anonymized_data/cache/cached_bert_train_multi_label_512_train_final.csv\n",
      "01/25/2022 18:43:03 - INFO - root -   Writing example 0 of 1065\n",
      "01/25/2022 18:43:03 - INFO - root -   Saving features into cached file transformer-EV-topic-classification/anonymized_data/cache/cached_bert_dev_multi_label_512_valid_final.csv\n",
      "01/25/2022 18:43:03 - INFO - root -   Writing example 0 of 14\n",
      "01/25/2022 18:43:03 - INFO - root -   Saving features into cached file transformer-EV-topic-classification/anonymized_data/cache/cached_bert_test_multi_label_512_test\n",
      "8\n",
      "cuda:0\n",
      "01/25/2022 18:43:03 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/azure12ev22/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
      "01/25/2022 18:43:03 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "01/25/2022 18:43:03 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/bert-base-cased-pytorch_model.bin from cache at /home/azure12ev22/.cache/torch/transformers/d8f11f061e407be64c4d5d7867ee61d1465263e24085cfa26abf183fdc830569.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
      "01/25/2022 18:43:07 - WARNING - transformers.modeling_utils -   Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMultiLabelSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "01/25/2022 18:43:07 - WARNING - transformers.modeling_utils -   Some weights of BertForMultiLabelSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "01/25/2022 18:43:10 - INFO - root -   ***** Running training *****\n",
      "01/25/2022 18:43:10 - INFO - root -     Num examples = 8521\n",
      "01/25/2022 18:43:10 - INFO - root -     Num Epochs = 20\n",
      "01/25/2022 18:43:10 - INFO - root -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "01/25/2022 18:43:10 - INFO - root -     Gradient Accumulation steps = 1\n",
      "01/25/2022 18:43:10 - INFO - root -     Total optimization steps = 10660\n",
      "/home/azure12ev22/anaconda3/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/azure12ev22/anaconda3/lib/python3.8/site-packages/pytorch_lamb/lamb.py:96: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "01/25/2022 18:46:50 - INFO - root -   Running evaluation         \n",
      "01/25/2022 18:46:50 - INFO - root -     Num examples = 1065\n",
      "01/25/2022 18:46:50 - INFO - root -     Batch size = 32\n",
      "01/25/2022 18:46:57 - INFO - root -   eval_loss after epoch 1: 0.4270106773166096: \n",
      "01/25/2022 18:46:57 - INFO - root -   eval_accuracy after epoch 1: 0.0: \n",
      "01/25/2022 18:46:57 - INFO - root -   eval_accuracy_thresh after epoch 1: 0.8259389996528625: \n",
      "01/25/2022 18:46:57 - INFO - root -   eval_roc_auc after epoch 1: 0.7650471220028742: \n",
      "01/25/2022 18:46:57 - INFO - root -   eval_fbeta after epoch 1: 0.5530173778533936: \n",
      "/home/azure12ev22/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "01/25/2022 18:46:57 - INFO - root -   lr after epoch 1: 9.999739698632535e-05\n",
      "01/25/2022 18:46:57 - INFO - root -   train_loss after epoch 1: 0.5747172414250042\n",
      "01/25/2022 18:46:57 - INFO - root -   \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/25/2022 18:50:35 - INFO - root -   Running evaluation         \n",
      "01/25/2022 18:50:35 - INFO - root -     Num examples = 1065\n",
      "01/25/2022 18:50:35 - INFO - root -     Batch size = 32\n",
      "01/25/2022 18:50:42 - INFO - root -   eval_loss after epoch 2: 0.342749008361031: \n",
      "01/25/2022 18:50:42 - INFO - root -   eval_accuracy after epoch 2: 0.0: \n",
      "01/25/2022 18:50:42 - INFO - root -   eval_accuracy_thresh after epoch 2: 0.8658450841903687: \n",
      "01/25/2022 18:50:42 - INFO - root -   eval_roc_auc after epoch 2: 0.8627560050568899: \n",
      "01/25/2022 18:50:42 - INFO - root -   eval_fbeta after epoch 2: 0.6524176001548767: \n",
      "01/25/2022 18:50:42 - INFO - root -   lr after epoch 2: 9.923620574858906e-05\n",
      "01/25/2022 18:50:42 - INFO - root -   train_loss after epoch 2: 0.3967722256791972\n",
      "01/25/2022 18:50:42 - INFO - root -   \n",
      "\n",
      "01/25/2022 18:54:19 - INFO - root -   Running evaluation         \n",
      "01/25/2022 18:54:19 - INFO - root -     Num examples = 1065\n",
      "01/25/2022 18:54:19 - INFO - root -     Batch size = 32\n",
      "01/25/2022 18:54:26 - INFO - root -   eval_loss after epoch 3: 0.2825489868136013: \n",
      "01/25/2022 18:54:26 - INFO - root -   eval_accuracy after epoch 3: 0.0: \n",
      "01/25/2022 18:54:26 - INFO - root -   eval_accuracy_thresh after epoch 3: 0.8922535181045532: \n",
      "01/25/2022 18:54:26 - INFO - root -   eval_roc_auc after epoch 3: 0.9146187774860342: \n",
      "01/25/2022 18:54:26 - INFO - root -   eval_fbeta after epoch 3: 0.7466368675231934: \n",
      "01/25/2022 18:54:26 - INFO - root -   lr after epoch 3: 9.714066971576419e-05\n",
      "01/25/2022 18:54:26 - INFO - root -   train_loss after epoch 3: 0.3256399609414095\n",
      "01/25/2022 18:54:26 - INFO - root -   \n",
      "\n",
      "01/25/2022 18:58:04 - INFO - root -   Running evaluation         \n",
      "01/25/2022 18:58:04 - INFO - root -     Num examples = 1065\n",
      "01/25/2022 18:58:04 - INFO - root -     Batch size = 32\n",
      "01/25/2022 18:58:11 - INFO - root -   eval_loss after epoch 4: 0.24794411089490442: \n",
      "01/25/2022 18:58:11 - INFO - root -   eval_accuracy after epoch 4: 0.0: \n",
      "01/25/2022 18:58:11 - INFO - root -   eval_accuracy_thresh after epoch 4: 0.904342770576477: \n",
      "01/25/2022 18:58:11 - INFO - root -   eval_roc_auc after epoch 4: 0.9354054480426162: \n",
      "01/25/2022 18:58:11 - INFO - root -   eval_fbeta after epoch 4: 0.7863723039627075: \n",
      "01/25/2022 18:58:11 - INFO - root -   lr after epoch 4: 9.376757977081594e-05\n",
      "01/25/2022 18:58:11 - INFO - root -   train_loss after epoch 4: 0.2724993417902691\n",
      "01/25/2022 18:58:11 - INFO - root -   \n",
      "\n",
      "01/25/2022 19:01:47 - INFO - root -   Running evaluation         \n",
      "01/25/2022 19:01:47 - INFO - root -     Num examples = 1065\n",
      "01/25/2022 19:01:47 - INFO - root -     Batch size = 32\n",
      "01/25/2022 19:01:54 - INFO - root -   eval_loss after epoch 5: 0.23118959454929128: \n",
      "01/25/2022 19:01:54 - INFO - root -   eval_accuracy after epoch 5: 0.0: \n",
      "01/25/2022 19:01:54 - INFO - root -   eval_accuracy_thresh after epoch 5: 0.9091549515724182: \n",
      "01/25/2022 19:01:54 - INFO - root -   eval_roc_auc after epoch 5: 0.9426080153867764: \n",
      "01/25/2022 19:01:54 - INFO - root -   eval_fbeta after epoch 5: 0.8041439056396484: \n",
      "01/25/2022 19:01:54 - INFO - root -   lr after epoch 5: 8.920834963953769e-05\n",
      "01/25/2022 19:01:54 - INFO - root -   train_loss after epoch 5: 0.23918296623185248\n",
      "01/25/2022 19:01:54 - INFO - root -   \n",
      "\n",
      "01/25/2022 19:05:31 - INFO - root -   Running evaluation         \n",
      "01/25/2022 19:05:31 - INFO - root -     Num examples = 1065\n",
      "01/25/2022 19:05:31 - INFO - root -     Batch size = 32\n",
      "01/25/2022 19:05:38 - INFO - root -   eval_loss after epoch 6: 0.21979140577947393: \n",
      "01/25/2022 19:05:38 - INFO - root -   eval_accuracy after epoch 6: 0.0: \n",
      "01/25/2022 19:05:38 - INFO - root -   eval_accuracy_thresh after epoch 6: 0.9119718670845032: \n",
      "01/25/2022 19:05:38 - INFO - root -   eval_roc_auc after epoch 6: 0.9477753789966179: \n",
      "01/25/2022 19:05:38 - INFO - root -   eval_fbeta after epoch 6: 0.8177393078804016: \n",
      "01/25/2022 19:05:38 - INFO - root -   lr after epoch 6: 8.358653849759678e-05\n",
      "01/25/2022 19:05:38 - INFO - root -   train_loss after epoch 6: 0.2164658311943176\n",
      "01/25/2022 19:05:38 - INFO - root -   \n",
      "\n",
      "01/25/2022 19:09:15 - INFO - root -   Running evaluation         \n",
      "01/25/2022 19:09:15 - INFO - root -     Num examples = 1065\n",
      "01/25/2022 19:09:15 - INFO - root -     Batch size = 32\n",
      "01/25/2022 19:09:22 - INFO - root -   eval_loss after epoch 7: 0.21474685739068425: \n",
      "01/25/2022 19:09:22 - INFO - root -   eval_accuracy after epoch 7: 0.0: \n",
      "01/25/2022 19:09:22 - INFO - root -   eval_accuracy_thresh after epoch 7: 0.9159624576568604: \n",
      "01/25/2022 19:09:22 - INFO - root -   eval_roc_auc after epoch 7: 0.9496367467340918: \n",
      "01/25/2022 19:09:22 - INFO - root -   eval_fbeta after epoch 7: 0.8282897472381592: \n",
      "01/25/2022 19:09:22 - INFO - root -   lr after epoch 7: 7.705450240741043e-05\n",
      "01/25/2022 19:09:22 - INFO - root -   train_loss after epoch 7: 0.196841946448327\n",
      "01/25/2022 19:09:22 - INFO - root -   \n",
      "\n",
      "01/25/2022 19:12:59 - INFO - root -   Running evaluation         \n",
      "01/25/2022 19:12:59 - INFO - root -     Num examples = 1065\n",
      "01/25/2022 19:12:59 - INFO - root -     Batch size = 32\n",
      "01/25/2022 19:13:06 - INFO - root -   eval_loss after epoch 8: 0.21641992996720708: \n",
      "01/25/2022 19:13:06 - INFO - root -   eval_accuracy after epoch 8: 0.0: \n",
      "01/25/2022 19:13:06 - INFO - root -   eval_accuracy_thresh after epoch 8: 0.9146713614463806: \n",
      "01/25/2022 19:13:06 - INFO - root -   eval_roc_auc after epoch 8: 0.94892173706333: \n",
      "01/25/2022 19:13:06 - INFO - root -   eval_fbeta after epoch 8: 0.8218925595283508: \n",
      "01/25/2022 19:13:06 - INFO - root -   lr after epoch 8: 6.978926533387911e-05\n",
      "01/25/2022 19:13:06 - INFO - root -   train_loss after epoch 8: 0.1831178949345567\n",
      "01/25/2022 19:13:06 - INFO - root -   \n",
      "\n",
      "01/25/2022 19:16:43 - INFO - root -   Running evaluation         \n",
      "01/25/2022 19:16:43 - INFO - root -     Num examples = 1065\n",
      "01/25/2022 19:16:43 - INFO - root -     Batch size = 32\n",
      "01/25/2022 19:16:50 - INFO - root -   eval_loss after epoch 9: 0.2128624035155072: \n",
      "01/25/2022 19:16:50 - INFO - root -   eval_accuracy after epoch 9: 0.0: \n",
      "01/25/2022 19:16:50 - INFO - root -   eval_accuracy_thresh after epoch 9: 0.9147887825965881: \n",
      "01/25/2022 19:16:50 - INFO - root -   eval_roc_auc after epoch 9: 0.9515035603531178: \n",
      "01/25/2022 19:16:50 - INFO - root -   eval_fbeta after epoch 9: 0.8256168961524963: \n",
      "01/25/2022 19:16:50 - INFO - root -   lr after epoch 9: 6.198772163810884e-05\n",
      "01/25/2022 19:16:50 - INFO - root -   train_loss after epoch 9: 0.1686721108224781\n",
      "01/25/2022 19:16:50 - INFO - root -   \n",
      "\n",
      "01/25/2022 19:20:27 - INFO - root -   Running evaluation          \n",
      "01/25/2022 19:20:27 - INFO - root -     Num examples = 1065\n",
      "01/25/2022 19:20:27 - INFO - root -     Batch size = 32\n",
      "01/25/2022 19:20:34 - INFO - root -   eval_loss after epoch 10: 0.21598149748409495: \n",
      "01/25/2022 19:20:34 - INFO - root -   eval_accuracy after epoch 10: 0.0: \n",
      "01/25/2022 19:20:34 - INFO - root -   eval_accuracy_thresh after epoch 10: 0.9149061441421509: \n",
      "01/25/2022 19:20:34 - INFO - root -   eval_roc_auc after epoch 10: 0.9510716068592173: \n",
      "01/25/2022 19:20:34 - INFO - root -   eval_fbeta after epoch 10: 0.8257949352264404: \n",
      "01/25/2022 19:20:34 - INFO - root -   lr after epoch 10: 5.38613000657926e-05\n",
      "01/25/2022 19:20:34 - INFO - root -   train_loss after epoch 10: 0.15859776768854367\n",
      "01/25/2022 19:20:34 - INFO - root -   \n",
      "\n",
      "01/25/2022 19:24:10 - INFO - root -   Running evaluation          \n",
      "01/25/2022 19:24:10 - INFO - root -     Num examples = 1065\n",
      "01/25/2022 19:24:10 - INFO - root -     Batch size = 32\n",
      "01/25/2022 19:24:17 - INFO - root -   eval_loss after epoch 11: 0.21689043194055557: \n",
      "01/25/2022 19:24:17 - INFO - root -   eval_accuracy after epoch 11: 0.0: \n",
      "01/25/2022 19:24:17 - INFO - root -   eval_accuracy_thresh after epoch 11: 0.9152582287788391: \n",
      "01/25/2022 19:24:17 - INFO - root -   eval_roc_auc after epoch 11: 0.9511456881368386: \n",
      "01/25/2022 19:24:17 - INFO - root -   eval_fbeta after epoch 11: 0.8226422667503357: \n",
      "01/25/2022 19:24:17 - INFO - root -   lr after epoch 11: 4.5630233840893374e-05\n",
      "01/25/2022 19:24:17 - INFO - root -   train_loss after epoch 11: 0.1480276895718436\n",
      "01/25/2022 19:24:17 - INFO - root -   \n",
      "\n",
      "01/25/2022 19:27:54 - INFO - root -   Running evaluation          \n",
      "01/25/2022 19:27:54 - INFO - root -     Num examples = 1065\n",
      "01/25/2022 19:27:54 - INFO - root -     Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/25/2022 19:28:01 - INFO - root -   eval_loss after epoch 12: 0.21668192481293397: \n",
      "01/25/2022 19:28:01 - INFO - root -   eval_accuracy after epoch 12: 0.0: \n",
      "01/25/2022 19:28:01 - INFO - root -   eval_accuracy_thresh after epoch 12: 0.9156103730201721: \n",
      "01/25/2022 19:28:01 - INFO - root -   eval_roc_auc after epoch 12: 0.9522065112861572: \n",
      "01/25/2022 19:28:01 - INFO - root -   eval_fbeta after epoch 12: 0.8243410587310791: \n",
      "01/25/2022 19:28:01 - INFO - root -   lr after epoch 12: 3.751759215016583e-05\n",
      "01/25/2022 19:28:01 - INFO - root -   train_loss after epoch 12: 0.1407757729753209\n",
      "01/25/2022 19:28:01 - INFO - root -   \n",
      "\n",
      "01/25/2022 19:31:37 - INFO - root -   Running evaluation          \n",
      "01/25/2022 19:31:37 - INFO - root -     Num examples = 1065\n",
      "01/25/2022 19:31:37 - INFO - root -     Batch size = 32\n",
      "01/25/2022 19:31:44 - INFO - root -   eval_loss after epoch 13: 0.21715784905587926: \n",
      "01/25/2022 19:31:44 - INFO - root -   eval_accuracy after epoch 13: 0.0: \n",
      "01/25/2022 19:31:44 - INFO - root -   eval_accuracy_thresh after epoch 13: 0.9160798192024231: \n",
      "01/25/2022 19:31:44 - INFO - root -   eval_roc_auc after epoch 13: 0.9517324170421516: \n",
      "01/25/2022 19:31:44 - INFO - root -   eval_fbeta after epoch 13: 0.824242353439331: \n",
      "01/25/2022 19:31:44 - INFO - root -   lr after epoch 13: 2.9743234770574034e-05\n",
      "01/25/2022 19:31:44 - INFO - root -   train_loss after epoch 13: 0.13409404127420002\n",
      "01/25/2022 19:31:44 - INFO - root -   \n",
      "\n",
      "01/25/2022 19:35:22 - INFO - root -   Running evaluation          \n",
      "01/25/2022 19:35:22 - INFO - root -     Num examples = 1065\n",
      "01/25/2022 19:35:22 - INFO - root -     Batch size = 32\n",
      "01/25/2022 19:35:29 - INFO - root -   eval_loss after epoch 14: 0.2206651327364585: \n",
      "01/25/2022 19:35:29 - INFO - root -   eval_accuracy after epoch 14: 0.0: \n",
      "01/25/2022 19:35:29 - INFO - root -   eval_accuracy_thresh after epoch 14: 0.9170188307762146: \n",
      "01/25/2022 19:35:29 - INFO - root -   eval_roc_auc after epoch 14: 0.9507305909429803: \n",
      "01/25/2022 19:35:29 - INFO - root -   eval_fbeta after epoch 14: 0.8268077373504639: \n",
      "01/25/2022 19:35:29 - INFO - root -   lr after epoch 14: 2.2517853674557698e-05\n",
      "01/25/2022 19:35:29 - INFO - root -   train_loss after epoch 14: 0.12948586754011615\n",
      "01/25/2022 19:35:29 - INFO - root -   \n",
      "\n",
      "01/25/2022 19:39:07 - INFO - root -   Running evaluation          \n",
      "01/25/2022 19:39:07 - INFO - root -     Num examples = 1065\n",
      "01/25/2022 19:39:07 - INFO - root -     Batch size = 32\n",
      "01/25/2022 19:39:14 - INFO - root -   eval_loss after epoch 15: 0.2219075884889154: \n",
      "01/25/2022 19:39:14 - INFO - root -   eval_accuracy after epoch 15: 0.0: \n",
      "01/25/2022 19:39:14 - INFO - root -   eval_accuracy_thresh after epoch 15: 0.9153755903244019: \n",
      "01/25/2022 19:39:14 - INFO - root -   eval_roc_auc after epoch 15: 0.950613374825764: \n",
      "01/25/2022 19:39:14 - INFO - root -   eval_fbeta after epoch 15: 0.8261185884475708: \n",
      "01/25/2022 19:39:14 - INFO - root -   lr after epoch 15: 1.6037263090922948e-05\n",
      "01/25/2022 19:39:14 - INFO - root -   train_loss after epoch 15: 0.12426202041123195\n",
      "01/25/2022 19:39:14 - INFO - root -   \n",
      "\n",
      "01/25/2022 19:42:51 - INFO - root -   Running evaluation          \n",
      "01/25/2022 19:42:51 - INFO - root -     Num examples = 1065\n",
      "01/25/2022 19:42:51 - INFO - root -     Batch size = 32\n",
      "01/25/2022 19:42:58 - INFO - root -   eval_loss after epoch 16: 0.2231564999503248: \n",
      "01/25/2022 19:42:58 - INFO - root -   eval_accuracy after epoch 16: 0.0: \n",
      "01/25/2022 19:42:58 - INFO - root -   eval_accuracy_thresh after epoch 16: 0.9153755903244019: \n",
      "01/25/2022 19:42:58 - INFO - root -   eval_roc_auc after epoch 16: 0.9502779344549256: \n",
      "01/25/2022 19:42:58 - INFO - root -   eval_fbeta after epoch 16: 0.8238377571105957: \n",
      "01/25/2022 19:42:58 - INFO - root -   lr after epoch 16: 1.0477092765766162e-05\n",
      "01/25/2022 19:42:58 - INFO - root -   train_loss after epoch 16: 0.12092899127844127\n",
      "01/25/2022 19:42:58 - INFO - root -   \n",
      "\n",
      "01/25/2022 19:46:37 - INFO - root -   Running evaluation          \n",
      "01/25/2022 19:46:37 - INFO - root -     Num examples = 1065\n",
      "01/25/2022 19:46:37 - INFO - root -     Batch size = 32\n",
      "01/25/2022 19:46:44 - INFO - root -   eval_loss after epoch 17: 0.22332661598920822: \n",
      "01/25/2022 19:46:44 - INFO - root -   eval_accuracy after epoch 17: 0.0: \n",
      "01/25/2022 19:46:44 - INFO - root -   eval_accuracy_thresh after epoch 17: 0.9153755903244019: \n",
      "01/25/2022 19:46:44 - INFO - root -   eval_roc_auc after epoch 17: 0.9506699947053929: \n",
      "01/25/2022 19:46:44 - INFO - root -   eval_fbeta after epoch 17: 0.8252432942390442: \n",
      "01/25/2022 19:46:44 - INFO - root -   lr after epoch 17: 5.988028240760935e-06\n",
      "01/25/2022 19:46:44 - INFO - root -   train_loss after epoch 17: 0.11961840132275844\n",
      "01/25/2022 19:46:44 - INFO - root -   \n",
      "\n",
      "01/25/2022 19:50:22 - INFO - root -   Running evaluation          \n",
      "01/25/2022 19:50:22 - INFO - root -     Num examples = 1065\n",
      "01/25/2022 19:50:22 - INFO - root -     Batch size = 32\n",
      "01/25/2022 19:50:29 - INFO - root -   eval_loss after epoch 18: 0.22347119888838599: \n",
      "01/25/2022 19:50:29 - INFO - root -   eval_accuracy after epoch 18: 0.0: \n",
      "01/25/2022 19:50:29 - INFO - root -   eval_accuracy_thresh after epoch 18: 0.9158450961112976: \n",
      "01/25/2022 19:50:29 - INFO - root -   eval_roc_auc after epoch 18: 0.9506235750483537: \n",
      "01/25/2022 19:50:29 - INFO - root -   eval_fbeta after epoch 18: 0.8248231410980225: \n",
      "01/25/2022 19:50:29 - INFO - root -   lr after epoch 18: 2.6917271414835297e-06\n",
      "01/25/2022 19:50:29 - INFO - root -   train_loss after epoch 18: 0.11817594458249452\n",
      "01/25/2022 19:50:29 - INFO - root -   \n",
      "\n",
      "01/25/2022 19:54:07 - INFO - root -   Running evaluation          \n",
      "01/25/2022 19:54:07 - INFO - root -     Num examples = 1065\n",
      "01/25/2022 19:54:07 - INFO - root -     Batch size = 32\n",
      "01/25/2022 19:54:14 - INFO - root -   eval_loss after epoch 19: 0.22391018972677343: \n",
      "01/25/2022 19:54:14 - INFO - root -   eval_accuracy after epoch 19: 0.0: \n",
      "01/25/2022 19:54:14 - INFO - root -   eval_accuracy_thresh after epoch 19: 0.9158450961112976: \n",
      "01/25/2022 19:54:14 - INFO - root -   eval_roc_auc after epoch 19: 0.9505500124261186: \n",
      "01/25/2022 19:54:14 - INFO - root -   eval_fbeta after epoch 19: 0.8252382278442383: \n",
      "01/25/2022 19:54:14 - INFO - root -   lr after epoch 19: 6.775221479809302e-07\n",
      "01/25/2022 19:54:14 - INFO - root -   train_loss after epoch 19: 0.1169289724995674\n",
      "01/25/2022 19:54:14 - INFO - root -   \n",
      "\n",
      "01/25/2022 19:57:51 - INFO - root -   Running evaluation          \n",
      "01/25/2022 19:57:51 - INFO - root -     Num examples = 1065\n",
      "01/25/2022 19:57:51 - INFO - root -     Batch size = 32\n",
      "01/25/2022 19:57:58 - INFO - root -   eval_loss after epoch 20: 0.22381938511834426: \n",
      "01/25/2022 19:57:58 - INFO - root -   eval_accuracy after epoch 20: 0.0: \n",
      "01/25/2022 19:57:58 - INFO - root -   eval_accuracy_thresh after epoch 20: 0.9158450961112976: \n",
      "01/25/2022 19:57:58 - INFO - root -   eval_roc_auc after epoch 20: 0.9505780198169579: \n",
      "01/25/2022 19:57:58 - INFO - root -   eval_fbeta after epoch 20: 0.8251922130584717: \n",
      "01/25/2022 19:57:58 - INFO - root -   lr after epoch 20: 0.0\n",
      "01/25/2022 19:57:58 - INFO - root -   train_loss after epoch 20: 0.11596767629512479\n",
      "01/25/2022 19:57:58 - INFO - root -   \n",
      "\n",
      "01/25/2022 19:57:58 - INFO - root -   Running evaluation\n",
      "01/25/2022 19:57:58 - INFO - root -     Num examples = 1065\n",
      "01/25/2022 19:57:58 - INFO - root -     Batch size = 32\n",
      "01/25/2022 19:58:05 - INFO - transformers.configuration_utils -   Configuration saved in transformer-EV-topic-classification/transformers/models/output/model_out/config.json\n",
      "01/25/2022 19:58:06 - INFO - transformers.modeling_utils -   Model weights saved in transformer-EV-topic-classification/transformers/models/output/model_out/pytorch_model.bin\n",
      "01/25/2022 19:58:06 - INFO - root -   ---PROGRESS-STATUS---: Tokenizing input texts...\n",
      "01/25/2022 19:58:06 - INFO - root -   Writing example 0 of 1066\n",
      "01/25/2022 19:58:06 - INFO - root -   ---PROGRESS-STATUS---: Tokenizing input texts...DONE\n",
      "01/25/2022 19:58:06 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 1/34\n",
      "01/25/2022 19:58:06 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 2/34\n",
      "01/25/2022 19:58:07 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 3/34\n",
      "01/25/2022 19:58:07 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 4/34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/25/2022 19:58:07 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 5/34\n",
      "01/25/2022 19:58:07 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 6/34\n",
      "01/25/2022 19:58:07 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 7/34\n",
      "01/25/2022 19:58:08 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 8/34\n",
      "01/25/2022 19:58:08 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 9/34\n",
      "01/25/2022 19:58:08 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 10/34\n",
      "01/25/2022 19:58:08 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 11/34\n",
      "01/25/2022 19:58:08 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 12/34\n",
      "01/25/2022 19:58:09 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 13/34\n",
      "01/25/2022 19:58:09 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 14/34\n",
      "01/25/2022 19:58:09 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 15/34\n",
      "01/25/2022 19:58:09 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 16/34\n",
      "01/25/2022 19:58:09 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 17/34\n",
      "01/25/2022 19:58:10 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 18/34\n",
      "01/25/2022 19:58:10 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 19/34\n",
      "01/25/2022 19:58:10 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 20/34\n",
      "01/25/2022 19:58:10 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 21/34\n",
      "01/25/2022 19:58:10 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 22/34\n",
      "01/25/2022 19:58:11 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 23/34\n",
      "01/25/2022 19:58:11 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 24/34\n",
      "01/25/2022 19:58:11 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 25/34\n",
      "01/25/2022 19:58:11 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 26/34\n",
      "01/25/2022 19:58:11 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 27/34\n",
      "01/25/2022 19:58:12 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 28/34\n",
      "01/25/2022 19:58:12 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 29/34\n",
      "01/25/2022 19:58:12 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 30/34\n",
      "01/25/2022 19:58:12 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 31/34\n",
      "01/25/2022 19:58:13 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 32/34\n",
      "01/25/2022 19:58:13 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 33/34\n",
      "01/25/2022 19:58:13 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 34/34\n",
      "01/25/2022 19:58:13 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch...DONE\n",
      "Training time : 4487.971405029297\n",
      "Prediction time : 7.075535774230957\n"
     ]
    }
   ],
   "source": [
    "!python transformer-EV-topic-classification/transformers/code/train_bert.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "transformer-EV-topic-classification/transformers/code/train_xlnet.py:30: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n",
      "01/26/2022 02:42:30 - INFO - root -   {'run_text': 'multilabel topic classification with freezable layers', 'train_size': -1, 'val_size': -1, 'log_path': PosixPath('transformer-EV-topic-classification/transformers/logs'), 'full_data_dir': PosixPath('transformer-EV-topic-classification/anonymized_data'), 'data_dir': PosixPath('transformer-EV-topic-classification/anonymized_data'), 'task_name': 'topics', 'no_cuda': False, 'bert_model': None, 'output_dir': PosixPath('transformer-EV-topic-classification/transformers/models/output'), 'max_seq_length': 512, 'do_train': True, 'do_eval': True, 'do_lower_case': True, 'train_batch_size': 8, 'eval_batch_size': 8, 'learning_rate': 0.0001, 'num_train_epochs': 20, 'warmup_proportion': 0.0, 'local_rank': -1, 'seed': 42, 'gradient_accumulation_steps': 1, 'optimize_on_cpu': False, 'fp16': False, 'fp16_opt_level': 'O1', 'weight_decay': 0.01, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'max_steps': -1, 'warmup_steps': 500, 'logging_steps': 50, 'eval_all_checkpoints': True, 'overwrite_output_dir': True, 'overwrite_cache': False, 'loss_scale': 128, 'model_name': 'xlnet-base-cased', 'model_type': 'xlnet'}\n",
      "01/26/2022 02:42:30 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-config.json from cache at /home/azure12ev22/.cache/torch/transformers/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.69e5e35e0b798cab5e473f253752f8bf4d280ee37682281a23eed80f6e2d09c6\n",
      "01/26/2022 02:42:30 - INFO - transformers.configuration_utils -   Model config XLNetConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLNetLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_type\": \"bi\",\n",
      "  \"bi_data\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"clamp_len\": -1,\n",
      "  \"d_head\": 64,\n",
      "  \"d_inner\": 3072,\n",
      "  \"d_model\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"end_n_top\": 5,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ff_activation\": \"gelu\",\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mem_len\": null,\n",
      "  \"model_type\": \"xlnet\",\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"pad_token_id\": 5,\n",
      "  \"reuse_len\": null,\n",
      "  \"same_length\": false,\n",
      "  \"start_n_top\": 5,\n",
      "  \"summary_activation\": \"tanh\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"last\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 250\n",
      "    }\n",
      "  },\n",
      "  \"untie_r\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "01/26/2022 02:42:30 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-spiece.model from cache at /home/azure12ev22/.cache/torch/transformers/dad589d582573df0293448af5109cb6981ca77239ed314e15ca63b7b8a318ddd.8b10bd978b5d01c21303cc761fc9ecd464419b3bf921864a355ba807cfbfafa8\n",
      "01/26/2022 02:42:30 - INFO - root -   Loading features from cached file transformer-EV-topic-classification/anonymized_data/cache/cached_xlnet_train_multi_label_512_train_final.csv\n",
      "01/26/2022 02:42:33 - INFO - root -   Loading features from cached file transformer-EV-topic-classification/anonymized_data/cache/cached_xlnet_dev_multi_label_512_valid_final.csv\n",
      "01/26/2022 02:42:33 - INFO - root -   Loading features from cached file transformer-EV-topic-classification/anonymized_data/cache/cached_xlnet_test_multi_label_512_test\n",
      "8\n",
      "cuda:0\n",
      "01/26/2022 02:42:33 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-config.json from cache at /home/azure12ev22/.cache/torch/transformers/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.69e5e35e0b798cab5e473f253752f8bf4d280ee37682281a23eed80f6e2d09c6\n",
      "01/26/2022 02:42:33 - INFO - transformers.configuration_utils -   Model config XLNetConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLNetLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_type\": \"bi\",\n",
      "  \"bi_data\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"clamp_len\": -1,\n",
      "  \"d_head\": 64,\n",
      "  \"d_inner\": 3072,\n",
      "  \"d_model\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"end_n_top\": 5,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ff_activation\": \"gelu\",\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mem_len\": null,\n",
      "  \"model_type\": \"xlnet\",\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"pad_token_id\": 5,\n",
      "  \"reuse_len\": null,\n",
      "  \"same_length\": false,\n",
      "  \"start_n_top\": 5,\n",
      "  \"summary_activation\": \"tanh\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"last\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 250\n",
      "    }\n",
      "  },\n",
      "  \"untie_r\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "01/26/2022 02:42:33 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/xlnet-base-cased-pytorch_model.bin from cache at /home/azure12ev22/.cache/torch/transformers/33d6135fea0154c088449506a4c5f9553cb59b6fd040138417a7033af64bb8f9.7eac4fe898a021204e63c88c00ea68c60443c57f94b4bc3c02adbde6465745ac\n",
      "01/26/2022 02:42:40 - WARNING - transformers.modeling_utils -   Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultiLabelSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "01/26/2022 02:42:40 - WARNING - transformers.modeling_utils -   Some weights of XLNetForMultiLabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "01/26/2022 02:42:55 - INFO - root -   ***** Running training *****\n",
      "01/26/2022 02:42:55 - INFO - root -     Num examples = 8521\n",
      "01/26/2022 02:42:55 - INFO - root -     Num Epochs = 20\n",
      "01/26/2022 02:42:55 - INFO - root -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "01/26/2022 02:42:55 - INFO - root -     Gradient Accumulation steps = 1\n",
      "01/26/2022 02:42:55 - INFO - root -     Total optimization steps = 10660\n",
      "/home/azure12ev22/anaconda3/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/azure12ev22/anaconda3/lib/python3.8/site-packages/pytorch_lamb/lamb.py:96: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "01/26/2022 02:50:02 - INFO - root -   Running evaluation         \n",
      "01/26/2022 02:50:02 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 02:50:02 - INFO - root -     Batch size = 32\n",
      "01/26/2022 02:50:28 - INFO - root -   eval_loss after epoch 1: 0.3630722217700061: \n",
      "01/26/2022 02:50:28 - INFO - root -   eval_accuracy after epoch 1: 0.0: \n",
      "01/26/2022 02:50:28 - INFO - root -   eval_accuracy_thresh after epoch 1: 0.8550469875335693: \n",
      "01/26/2022 02:50:28 - INFO - root -   eval_roc_auc after epoch 1: 0.8409752450106431: \n",
      "01/26/2022 02:50:28 - INFO - root -   eval_fbeta after epoch 1: 0.6207305192947388: \n",
      "/home/azure12ev22/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "01/26/2022 02:50:28 - INFO - root -   lr after epoch 1: 9.999739698632535e-05\n",
      "01/26/2022 02:50:28 - INFO - root -   train_loss after epoch 1: 0.5126666742574431\n",
      "01/26/2022 02:50:28 - INFO - root -   \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/26/2022 02:57:32 - INFO - root -   Running evaluation         \n",
      "01/26/2022 02:57:32 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 02:57:32 - INFO - root -     Batch size = 32\n",
      "01/26/2022 02:57:57 - INFO - root -   eval_loss after epoch 2: 0.26073701986495185: \n",
      "01/26/2022 02:57:57 - INFO - root -   eval_accuracy after epoch 2: 0.0: \n",
      "01/26/2022 02:57:57 - INFO - root -   eval_accuracy_thresh after epoch 2: 0.8949530720710754: \n",
      "01/26/2022 02:57:57 - INFO - root -   eval_roc_auc after epoch 2: 0.9264138653873167: \n",
      "01/26/2022 02:57:57 - INFO - root -   eval_fbeta after epoch 2: 0.7650172114372253: \n",
      "01/26/2022 02:57:57 - INFO - root -   lr after epoch 2: 9.923620574858906e-05\n",
      "01/26/2022 02:57:57 - INFO - root -   train_loss after epoch 2: 0.3327099726191157\n",
      "01/26/2022 02:57:57 - INFO - root -   \n",
      "\n",
      "01/26/2022 03:05:01 - INFO - root -   Running evaluation         \n",
      "01/26/2022 03:05:01 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 03:05:01 - INFO - root -     Batch size = 32\n",
      "01/26/2022 03:05:26 - INFO - root -   eval_loss after epoch 3: 0.23176454708856695: \n",
      "01/26/2022 03:05:26 - INFO - root -   eval_accuracy after epoch 3: 0.0: \n",
      "01/26/2022 03:05:26 - INFO - root -   eval_accuracy_thresh after epoch 3: 0.9065728187561035: \n",
      "01/26/2022 03:05:26 - INFO - root -   eval_roc_auc after epoch 3: 0.9430680627140804: \n",
      "01/26/2022 03:05:26 - INFO - root -   eval_fbeta after epoch 3: 0.7964197397232056: \n",
      "01/26/2022 03:05:26 - INFO - root -   lr after epoch 3: 9.714066971576419e-05\n",
      "01/26/2022 03:05:26 - INFO - root -   train_loss after epoch 3: 0.2652423185825795\n",
      "01/26/2022 03:05:26 - INFO - root -   \n",
      "\n",
      "01/26/2022 03:12:30 - INFO - root -   Running evaluation         \n",
      "01/26/2022 03:12:30 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 03:12:30 - INFO - root -     Batch size = 32\n",
      "01/26/2022 03:12:55 - INFO - root -   eval_loss after epoch 4: 0.22357886328416712: \n",
      "01/26/2022 03:12:55 - INFO - root -   eval_accuracy after epoch 4: 0.0: \n",
      "01/26/2022 03:12:55 - INFO - root -   eval_accuracy_thresh after epoch 4: 0.9099765419960022: \n",
      "01/26/2022 03:12:55 - INFO - root -   eval_roc_auc after epoch 4: 0.9486937015786574: \n",
      "01/26/2022 03:12:55 - INFO - root -   eval_fbeta after epoch 4: 0.8114427328109741: \n",
      "01/26/2022 03:12:55 - INFO - root -   lr after epoch 4: 9.376757977081594e-05\n",
      "01/26/2022 03:12:55 - INFO - root -   train_loss after epoch 4: 0.23313333946813114\n",
      "01/26/2022 03:12:55 - INFO - root -   \n",
      "\n",
      "01/26/2022 03:19:59 - INFO - root -   Running evaluation         \n",
      "01/26/2022 03:19:59 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 03:19:59 - INFO - root -     Batch size = 32\n",
      "01/26/2022 03:20:24 - INFO - root -   eval_loss after epoch 5: 0.2119654145310907: \n",
      "01/26/2022 03:20:24 - INFO - root -   eval_accuracy after epoch 5: 0.0: \n",
      "01/26/2022 03:20:24 - INFO - root -   eval_accuracy_thresh after epoch 5: 0.9131455421447754: \n",
      "01/26/2022 03:20:24 - INFO - root -   eval_roc_auc after epoch 5: 0.9542252044906913: \n",
      "01/26/2022 03:20:24 - INFO - root -   eval_fbeta after epoch 5: 0.8199290037155151: \n",
      "01/26/2022 03:20:24 - INFO - root -   lr after epoch 5: 8.920834963953769e-05\n",
      "01/26/2022 03:20:24 - INFO - root -   train_loss after epoch 5: 0.21288865114130626\n",
      "01/26/2022 03:20:24 - INFO - root -   \n",
      "\n",
      "01/26/2022 03:27:29 - INFO - root -   Running evaluation         \n",
      "01/26/2022 03:27:29 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 03:27:29 - INFO - root -     Batch size = 32\n",
      "01/26/2022 03:27:54 - INFO - root -   eval_loss after epoch 6: 0.2114379305173369: \n",
      "01/26/2022 03:27:54 - INFO - root -   eval_accuracy after epoch 6: 0.0: \n",
      "01/26/2022 03:27:54 - INFO - root -   eval_accuracy_thresh after epoch 6: 0.9129108190536499: \n",
      "01/26/2022 03:27:54 - INFO - root -   eval_roc_auc after epoch 6: 0.9549031735226424: \n",
      "01/26/2022 03:27:54 - INFO - root -   eval_fbeta after epoch 6: 0.828513503074646: \n",
      "01/26/2022 03:27:54 - INFO - root -   lr after epoch 6: 8.358653849759678e-05\n",
      "01/26/2022 03:27:54 - INFO - root -   train_loss after epoch 6: 0.1984405220114044\n",
      "01/26/2022 03:27:54 - INFO - root -   \n",
      "\n",
      "01/26/2022 03:34:58 - INFO - root -   Running evaluation         \n",
      "01/26/2022 03:34:58 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 03:34:58 - INFO - root -     Batch size = 32\n",
      "01/26/2022 03:35:23 - INFO - root -   eval_loss after epoch 7: 0.21038544572451534: \n",
      "01/26/2022 03:35:23 - INFO - root -   eval_accuracy after epoch 7: 0.0: \n",
      "01/26/2022 03:35:23 - INFO - root -   eval_accuracy_thresh after epoch 7: 0.9130281805992126: \n",
      "01/26/2022 03:35:23 - INFO - root -   eval_roc_auc after epoch 7: 0.9553929570920722: \n",
      "01/26/2022 03:35:23 - INFO - root -   eval_fbeta after epoch 7: 0.8289834260940552: \n",
      "01/26/2022 03:35:23 - INFO - root -   lr after epoch 7: 7.705450240741043e-05\n",
      "01/26/2022 03:35:23 - INFO - root -   train_loss after epoch 7: 0.18400593787627417\n",
      "01/26/2022 03:35:23 - INFO - root -   \n",
      "\n",
      "01/26/2022 03:42:28 - INFO - root -   Running evaluation         \n",
      "01/26/2022 03:42:28 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 03:42:28 - INFO - root -     Batch size = 32\n",
      "01/26/2022 03:42:53 - INFO - root -   eval_loss after epoch 8: 0.20940378856133013: \n",
      "01/26/2022 03:42:53 - INFO - root -   eval_accuracy after epoch 8: 0.0: \n",
      "01/26/2022 03:42:53 - INFO - root -   eval_accuracy_thresh after epoch 8: 0.9157277345657349: \n",
      "01/26/2022 03:42:53 - INFO - root -   eval_roc_auc after epoch 8: 0.9572335786141095: \n",
      "01/26/2022 03:42:53 - INFO - root -   eval_fbeta after epoch 8: 0.8365873694419861: \n",
      "01/26/2022 03:42:53 - INFO - root -   lr after epoch 8: 6.978926533387911e-05\n",
      "01/26/2022 03:42:53 - INFO - root -   train_loss after epoch 8: 0.17488270416492369\n",
      "01/26/2022 03:42:53 - INFO - root -   \n",
      "\n",
      "01/26/2022 03:49:57 - INFO - root -   Running evaluation         \n",
      "01/26/2022 03:49:57 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 03:49:57 - INFO - root -     Batch size = 32\n",
      "01/26/2022 03:50:23 - INFO - root -   eval_loss after epoch 9: 0.21349912881851196: \n",
      "01/26/2022 03:50:23 - INFO - root -   eval_accuracy after epoch 9: 0.0: \n",
      "01/26/2022 03:50:23 - INFO - root -   eval_accuracy_thresh after epoch 9: 0.9161971807479858: \n",
      "01/26/2022 03:50:23 - INFO - root -   eval_roc_auc after epoch 9: 0.9557917166412742: \n",
      "01/26/2022 03:50:23 - INFO - root -   eval_fbeta after epoch 9: 0.8336901664733887: \n",
      "01/26/2022 03:50:23 - INFO - root -   lr after epoch 9: 6.198772163810884e-05\n",
      "01/26/2022 03:50:23 - INFO - root -   train_loss after epoch 9: 0.16290162195491745\n",
      "01/26/2022 03:50:23 - INFO - root -   \n",
      "\n",
      "01/26/2022 03:57:27 - INFO - root -   Running evaluation          \n",
      "01/26/2022 03:57:27 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 03:57:27 - INFO - root -     Batch size = 32\n",
      "01/26/2022 03:57:52 - INFO - root -   eval_loss after epoch 10: 0.21359270548119264: \n",
      "01/26/2022 03:57:52 - INFO - root -   eval_accuracy after epoch 10: 0.0: \n",
      "01/26/2022 03:57:52 - INFO - root -   eval_accuracy_thresh after epoch 10: 0.9146713614463806: \n",
      "01/26/2022 03:57:52 - INFO - root -   eval_roc_auc after epoch 10: 0.9562738932650438: \n",
      "01/26/2022 03:57:52 - INFO - root -   eval_fbeta after epoch 10: 0.8308693766593933: \n",
      "01/26/2022 03:57:52 - INFO - root -   lr after epoch 10: 5.38613000657926e-05\n",
      "01/26/2022 03:57:52 - INFO - root -   train_loss after epoch 10: 0.15588153700220206\n",
      "01/26/2022 03:57:52 - INFO - root -   \n",
      "\n",
      "01/26/2022 04:04:56 - INFO - root -   Running evaluation          \n",
      "01/26/2022 04:04:56 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 04:04:56 - INFO - root -     Batch size = 32\n",
      "01/26/2022 04:05:22 - INFO - root -   eval_loss after epoch 11: 0.2153616793015424: \n",
      "01/26/2022 04:05:22 - INFO - root -   eval_accuracy after epoch 11: 0.0: \n",
      "01/26/2022 04:05:22 - INFO - root -   eval_accuracy_thresh after epoch 11: 0.9157277345657349: \n",
      "01/26/2022 04:05:22 - INFO - root -   eval_roc_auc after epoch 11: 0.9567593547062573: \n",
      "01/26/2022 04:05:22 - INFO - root -   eval_fbeta after epoch 11: 0.827266275882721: \n",
      "01/26/2022 04:05:22 - INFO - root -   lr after epoch 11: 4.5630233840893374e-05\n",
      "01/26/2022 04:05:22 - INFO - root -   train_loss after epoch 11: 0.14790371690805143\n",
      "01/26/2022 04:05:22 - INFO - root -   \n",
      "\n",
      "01/26/2022 04:12:26 - INFO - root -   Running evaluation          \n",
      "01/26/2022 04:12:26 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 04:12:26 - INFO - root -     Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/26/2022 04:12:51 - INFO - root -   eval_loss after epoch 12: 0.2174356947926914: \n",
      "01/26/2022 04:12:51 - INFO - root -   eval_accuracy after epoch 12: 0.0: \n",
      "01/26/2022 04:12:51 - INFO - root -   eval_accuracy_thresh after epoch 12: 0.9159624576568604: \n",
      "01/26/2022 04:12:51 - INFO - root -   eval_roc_auc after epoch 12: 0.9563925789058533: \n",
      "01/26/2022 04:12:51 - INFO - root -   eval_fbeta after epoch 12: 0.8287657499313354: \n",
      "01/26/2022 04:12:51 - INFO - root -   lr after epoch 12: 3.751759215016583e-05\n",
      "01/26/2022 04:12:51 - INFO - root -   train_loss after epoch 12: 0.1416044691280509\n",
      "01/26/2022 04:12:51 - INFO - root -   \n",
      "\n",
      "01/26/2022 04:19:56 - INFO - root -   Running evaluation          \n",
      "01/26/2022 04:19:56 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 04:19:56 - INFO - root -     Batch size = 32\n",
      "01/26/2022 04:20:21 - INFO - root -   eval_loss after epoch 13: 0.21822885248590917: \n",
      "01/26/2022 04:20:21 - INFO - root -   eval_accuracy after epoch 13: 0.0: \n",
      "01/26/2022 04:20:21 - INFO - root -   eval_accuracy_thresh after epoch 13: 0.9154930114746094: \n",
      "01/26/2022 04:20:21 - INFO - root -   eval_roc_auc after epoch 13: 0.9564241304418295: \n",
      "01/26/2022 04:20:21 - INFO - root -   eval_fbeta after epoch 13: 0.8296058177947998: \n",
      "01/26/2022 04:20:21 - INFO - root -   lr after epoch 13: 2.9743234770574034e-05\n",
      "01/26/2022 04:20:21 - INFO - root -   train_loss after epoch 13: 0.13670327760432943\n",
      "01/26/2022 04:20:21 - INFO - root -   \n",
      "\n",
      "01/26/2022 04:27:25 - INFO - root -   Running evaluation          \n",
      "01/26/2022 04:27:25 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 04:27:25 - INFO - root -     Batch size = 32\n",
      "01/26/2022 04:27:50 - INFO - root -   eval_loss after epoch 14: 0.2215904514579212: \n",
      "01/26/2022 04:27:50 - INFO - root -   eval_accuracy after epoch 14: 0.0: \n",
      "01/26/2022 04:27:50 - INFO - root -   eval_accuracy_thresh after epoch 14: 0.9136150479316711: \n",
      "01/26/2022 04:27:50 - INFO - root -   eval_roc_auc after epoch 14: 0.9551312090073152: \n",
      "01/26/2022 04:27:50 - INFO - root -   eval_fbeta after epoch 14: 0.8290049433708191: \n",
      "01/26/2022 04:27:50 - INFO - root -   lr after epoch 14: 2.2517853674557698e-05\n",
      "01/26/2022 04:27:50 - INFO - root -   train_loss after epoch 14: 0.13241073306069812\n",
      "01/26/2022 04:27:50 - INFO - root -   \n",
      "\n",
      "01/26/2022 04:34:54 - INFO - root -   Running evaluation          \n",
      "01/26/2022 04:34:54 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 04:34:54 - INFO - root -     Batch size = 32\n",
      "01/26/2022 04:35:20 - INFO - root -   eval_loss after epoch 15: 0.22173120081424713: \n",
      "01/26/2022 04:35:20 - INFO - root -   eval_accuracy after epoch 15: 0.0: \n",
      "01/26/2022 04:35:20 - INFO - root -   eval_accuracy_thresh after epoch 15: 0.9157277345657349: \n",
      "01/26/2022 04:35:20 - INFO - root -   eval_roc_auc after epoch 15: 0.955715733627238: \n",
      "01/26/2022 04:35:20 - INFO - root -   eval_fbeta after epoch 15: 0.8251616358757019: \n",
      "01/26/2022 04:35:20 - INFO - root -   lr after epoch 15: 1.6037263090922948e-05\n",
      "01/26/2022 04:35:20 - INFO - root -   train_loss after epoch 15: 0.12850452972118223\n",
      "01/26/2022 04:35:20 - INFO - root -   \n",
      "\n",
      "01/26/2022 04:42:23 - INFO - root -   Running evaluation          \n",
      "01/26/2022 04:42:23 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 04:42:23 - INFO - root -     Batch size = 32\n",
      "01/26/2022 04:42:49 - INFO - root -   eval_loss after epoch 16: 0.2238008209011134: \n",
      "01/26/2022 04:42:49 - INFO - root -   eval_accuracy after epoch 16: 0.0: \n",
      "01/26/2022 04:42:49 - INFO - root -   eval_accuracy_thresh after epoch 16: 0.9153755903244019: \n",
      "01/26/2022 04:42:49 - INFO - root -   eval_roc_auc after epoch 16: 0.955322160631895: \n",
      "01/26/2022 04:42:49 - INFO - root -   eval_fbeta after epoch 16: 0.8284115195274353: \n",
      "01/26/2022 04:42:49 - INFO - root -   lr after epoch 16: 1.0477092765766162e-05\n",
      "01/26/2022 04:42:49 - INFO - root -   train_loss after epoch 16: 0.12542464268196368\n",
      "01/26/2022 04:42:49 - INFO - root -   \n",
      "\n",
      "01/26/2022 04:49:52 - INFO - root -   Running evaluation          \n",
      "01/26/2022 04:49:52 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 04:49:52 - INFO - root -     Batch size = 32\n",
      "01/26/2022 04:50:17 - INFO - root -   eval_loss after epoch 17: 0.22352532867123098: \n",
      "01/26/2022 04:50:17 - INFO - root -   eval_accuracy after epoch 17: 0.0: \n",
      "01/26/2022 04:50:17 - INFO - root -   eval_accuracy_thresh after epoch 17: 0.9160798192024231: \n",
      "01/26/2022 04:50:17 - INFO - root -   eval_roc_auc after epoch 17: 0.9553879434233417: \n",
      "01/26/2022 04:50:17 - INFO - root -   eval_fbeta after epoch 17: 0.8238611817359924: \n",
      "01/26/2022 04:50:17 - INFO - root -   lr after epoch 17: 5.988028240760935e-06\n",
      "01/26/2022 04:50:17 - INFO - root -   train_loss after epoch 17: 0.12330148748592185\n",
      "01/26/2022 04:50:17 - INFO - root -   \n",
      "\n",
      "01/26/2022 04:57:21 - INFO - root -   Running evaluation          \n",
      "01/26/2022 04:57:21 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 04:57:21 - INFO - root -     Batch size = 32\n",
      "01/26/2022 04:57:47 - INFO - root -   eval_loss after epoch 18: 0.22440642907338984: \n",
      "01/26/2022 04:57:47 - INFO - root -   eval_accuracy after epoch 18: 0.0: \n",
      "01/26/2022 04:57:47 - INFO - root -   eval_accuracy_thresh after epoch 18: 0.9165493249893188: \n",
      "01/26/2022 04:57:47 - INFO - root -   eval_roc_auc after epoch 18: 0.9551658724756069: \n",
      "01/26/2022 04:57:47 - INFO - root -   eval_fbeta after epoch 18: 0.8239659070968628: \n",
      "01/26/2022 04:57:47 - INFO - root -   lr after epoch 18: 2.6917271414835297e-06\n",
      "01/26/2022 04:57:47 - INFO - root -   train_loss after epoch 18: 0.12119057348197218\n",
      "01/26/2022 04:57:47 - INFO - root -   \n",
      "\n",
      "01/26/2022 05:04:50 - INFO - root -   Running evaluation          \n",
      "01/26/2022 05:04:50 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 05:04:50 - INFO - root -     Batch size = 32\n",
      "01/26/2022 05:05:15 - INFO - root -   eval_loss after epoch 19: 0.2242528217680314: \n",
      "01/26/2022 05:05:15 - INFO - root -   eval_accuracy after epoch 19: 0.0: \n",
      "01/26/2022 05:05:15 - INFO - root -   eval_accuracy_thresh after epoch 19: 0.9158450961112976: \n",
      "01/26/2022 05:05:15 - INFO - root -   eval_roc_auc after epoch 19: 0.9552763460728062: \n",
      "01/26/2022 05:05:15 - INFO - root -   eval_fbeta after epoch 19: 0.825251042842865: \n",
      "01/26/2022 05:05:15 - INFO - root -   lr after epoch 19: 6.775221479809302e-07\n",
      "01/26/2022 05:05:15 - INFO - root -   train_loss after epoch 19: 0.12168845181365621\n",
      "01/26/2022 05:05:15 - INFO - root -   \n",
      "\n",
      "01/26/2022 05:12:20 - INFO - root -   Running evaluation          \n",
      "01/26/2022 05:12:20 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 05:12:20 - INFO - root -     Batch size = 32\n",
      "01/26/2022 05:12:45 - INFO - root -   eval_loss after epoch 20: 0.22421605841201894: \n",
      "01/26/2022 05:12:45 - INFO - root -   eval_accuracy after epoch 20: 0.0: \n",
      "01/26/2022 05:12:45 - INFO - root -   eval_accuracy_thresh after epoch 20: 0.9157277345657349: \n",
      "01/26/2022 05:12:45 - INFO - root -   eval_roc_auc after epoch 20: 0.9552942396836204: \n",
      "01/26/2022 05:12:45 - INFO - root -   eval_fbeta after epoch 20: 0.8255621194839478: \n",
      "01/26/2022 05:12:45 - INFO - root -   lr after epoch 20: 0.0\n",
      "01/26/2022 05:12:45 - INFO - root -   train_loss after epoch 20: 0.11982556965768225\n",
      "01/26/2022 05:12:45 - INFO - root -   \n",
      "\n",
      "01/26/2022 05:12:45 - INFO - root -   Running evaluation\n",
      "01/26/2022 05:12:45 - INFO - root -     Num examples = 1065\n",
      "01/26/2022 05:12:45 - INFO - root -     Batch size = 32\n",
      "01/26/2022 05:13:10 - INFO - transformers.configuration_utils -   Configuration saved in transformer-EV-topic-classification/transformers/models/output/model_out/config.json\n",
      "01/26/2022 05:13:14 - INFO - transformers.modeling_utils -   Model weights saved in transformer-EV-topic-classification/transformers/models/output/model_out/pytorch_model.bin\n",
      "01/26/2022 05:13:14 - INFO - root -   ---PROGRESS-STATUS---: Tokenizing input texts...\n",
      "01/26/2022 05:13:14 - INFO - root -   Writing example 0 of 1066\n",
      "01/26/2022 05:13:14 - INFO - root -   ---PROGRESS-STATUS---: Tokenizing input texts...DONE\n",
      "01/26/2022 05:13:14 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 1/34\n",
      "01/26/2022 05:13:15 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 2/34\n",
      "01/26/2022 05:13:16 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 3/34\n",
      "01/26/2022 05:13:17 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 4/34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/26/2022 05:13:18 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 5/34\n",
      "01/26/2022 05:13:18 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 6/34\n",
      "01/26/2022 05:13:19 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 7/34\n",
      "01/26/2022 05:13:20 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 8/34\n",
      "01/26/2022 05:13:21 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 9/34\n",
      "01/26/2022 05:13:21 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 10/34\n",
      "01/26/2022 05:13:22 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 11/34\n",
      "01/26/2022 05:13:23 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 12/34\n",
      "01/26/2022 05:13:24 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 13/34\n",
      "01/26/2022 05:13:24 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 14/34\n",
      "01/26/2022 05:13:25 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 15/34\n",
      "01/26/2022 05:13:26 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 16/34\n",
      "01/26/2022 05:13:27 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 17/34\n",
      "01/26/2022 05:13:27 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 18/34\n",
      "01/26/2022 05:13:28 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 19/34\n",
      "01/26/2022 05:13:29 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 20/34\n",
      "01/26/2022 05:13:30 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 21/34\n",
      "01/26/2022 05:13:30 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 22/34\n",
      "01/26/2022 05:13:31 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 23/34\n",
      "01/26/2022 05:13:32 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 24/34\n",
      "01/26/2022 05:13:33 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 25/34\n",
      "01/26/2022 05:13:33 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 26/34\n",
      "01/26/2022 05:13:34 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 27/34\n",
      "01/26/2022 05:13:35 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 28/34\n",
      "01/26/2022 05:13:36 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 29/34\n",
      "01/26/2022 05:13:36 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 30/34\n",
      "01/26/2022 05:13:37 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 31/34\n",
      "01/26/2022 05:13:38 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 32/34\n",
      "01/26/2022 05:13:39 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 33/34\n",
      "01/26/2022 05:13:39 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch 34/34\n",
      "01/26/2022 05:13:40 - INFO - root -   ---PROGRESS-STATUS---: Predicting batch...DONE\n",
      "Training time : 8990.435151576996\n",
      "Prediction time : 25.867688417434692\n"
     ]
    }
   ],
   "source": [
    "!python transformer-EV-topic-classification/transformers/code/train_xlnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python transformer-EV-topic-classification/transformers/code/xlnet_inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"transformer-EV-topic-classification/transformers/code/bert_inference.py\", line 13, in <module>\r\n",
      "    from sklearn.model_selection import train_test_split\r\n",
      "ModuleNotFoundError: No module named 'sklearn'\r\n"
     ]
    }
   ],
   "source": [
    "!python transformer-EV-topic-classification/transformers/code/bert_inference.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
